{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -qq sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-13T10:39:31.354938Z","iopub.execute_input":"2024-04-13T10:39:31.355522Z","iopub.status.idle":"2024-04-13T10:39:46.623080Z","shell.execute_reply.started":"2024-04-13T10:39:31.355472Z","shell.execute_reply":"2024-04-13T10:39:46.621702Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Why Embeddings?**\nA significant hurdle for modern generative models is their inability to directly interact with your data. Consider a scenario where your task is to generate a report on recent market trends based on internal research documents. Traditional generative models fall short here as they don't have access to or understanding of your internal documents, making it impossible for them to generate the required report.\n\nTo address this challenge, the Retrieval-Augmented Generation (RAG) technique offers a solution. Imagine you have a repository of internal research on market trends. This repository can be processed through an embedding model to convert the documents into a searchable format within a vector database. When you need a report on market trends, the embedding model can locate and fetch the most relevant documents. These documents can then inform a generative model, enabling it to produce a detailed report based on your specific data.\n\n* https://www.mixedbread.ai/blog/mxbai-embed-large-v1\n\n\nall-MiniLM-L6-v2\n\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective. We used the pretrained nreimers/MiniLM-L6-H384-uncased model and fine-tuned in on a 1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\n**Task Variants**\n* Passage Ranking\n\nPassage Ranking is the task of ranking documents based on their relevance to a given query. The task is evaluated on Mean Reciprocal Rank. These models take one query and multiple documents and return ranked documents according to the relevancy to the query. 📄\n\n* Semantic Textual Similarity\n\nSemantic Textual Similarity is the task of evaluating how similar two texts are in terms of meaning. These models take a source sentence and a list of sentences in which we will look for similarities and will return a list of similarity scores. The benchmark dataset is the Semantic Textual Similarity Benchmark. The task is evaluated on Pearson’s Rank Correlation.\n\nText embeddings are versatile tools that can be applied to various natural language processing (NLP) tasks. Here are some common use cases:\n\nSemantic Search: Text embeddings can be used to represent documents or sentences in a high-dimensional space, enabling efficient similarity search. This is useful for tasks like document retrieval, finding similar documents, or searching for relevant information.\n\nInformation Retrieval: Embeddings can help improve search engine results by better understanding the semantics of queries and documents, leading to more relevant search results.\n\nText Classification: Embeddings can be fed into classification models to analyze and categorize text data. This is useful for sentiment analysis, topic classification, spam detection, and more.\n\nNamed Entity Recognition (NER): Embeddings can assist in identifying and classifying named entities (such as names of people, organizations, and locations) within text documents.\n\nMachine Translation: Embeddings can be used to represent words or sentences in different languages, aiding in the process of machine translation by capturing semantic similarities between words across languages.\n\nText Generation: Embeddings can be utilized in text generation tasks such as language modeling, dialogue generation, and text summarization to capture semantic information and generate coherent and contextually relevant text.\n\nSentiment Analysis: Embeddings can be employed to understand and classify the sentiment expressed in text data, helping businesses gauge public opinion, customer feedback, and brand sentiment.\n\nRecommendation Systems: Embeddings can be used to represent items (such as products, movies, or articles) and users in recommendation systems, enabling personalized recommendations based on similarity between item embeddings and user preferences.\n\nQuestion Answering: Embeddings can be used to match questions with relevant passages of text in large corpora, facilitating question answering systems that can retrieve accurate answers from unstructured text data.\n\nText Summarization: Embeddings can be used to identify important sentences or phrases within a document, aiding in the automatic summarization of text by extracting key information.\n\nCustomer Support Automation: Text embeddings can be employed in chatbots and virtual assistants to understand and respond to customer queries, providing automated customer support services.\n\nTopic Modeling: Embeddings can be used to cluster documents into topics or themes, helping researchers and analysts to identify patterns and trends within large text corpora.\n\nAbstractive Summarization: Embeddings can aid in generating abstractive summaries by capturing the semantic meaning of the input text and rephrasing it in a concise and coherent manner.\n\nText Similarity Detection: Embeddings can be used to measure the similarity between pairs of texts, enabling applications such as plagiarism detection, duplicate content detection, and paraphrase identification.\n\nText Annotation: Embeddings can assist in annotating text data with relevant tags or labels, facilitating tasks such as named entity recognition, part-of-speech tagging, and syntactic parsing.\n\nSocial Media Analytics: Text embeddings can be applied to analyze social media content, including sentiment analysis, trend detection, and identifying influential users or topics.\n\nLanguage Understanding in AI Assistants: Embeddings can help AI assistants understand user queries and commands in natural language, enabling more intuitive and effective human-computer interactions.\n\nTextual Entailment Recognition: Embeddings can be used to determine whether one text logically entails another, supporting tasks such as natural language inference and textual entailment recognition.\n\nText Clustering and Visualization: Embeddings can be utilized to cluster similar documents or sentences together and visualize them in a lower-dimensional space, aiding in exploratory data analysis and visualization of text data.\n\nText-Based Fraud Detection: Embeddings can assist in detecting fraudulent activities by analyzing patterns and anomalies in text data, such as identifying suspicious messages or transactions.\n\nThese examples illustrate the broad range of applications for text embeddings across various domains, from customer service and social media analytics to fraud detection and language understanding.\n\nText-to-Speech Synthesis: Embeddings can be used to generate natural-sounding speech from text input, by encoding the semantic information of the text and synthesizing corresponding speech signals.\n\nDocument Clustering for Information Retrieval: Embeddings can aid in clustering similar documents together, enabling more efficient information retrieval systems by organizing documents into meaningful groups.\n\nEvent Detection in News and Social Media: Embeddings can be applied to detect and classify events or incidents mentioned in news articles or social media posts, facilitating real-time event monitoring and analysis.\n\nText-Based Fraudulent Review Detection: Embeddings can assist in identifying fraudulent or fake reviews by analyzing the semantic content of reviews and detecting patterns indicative of fraudulent behavior.\n\nQuestion Generation: Embeddings can be used to generate questions from passages of text, aiding in the creation of educational materials, quizzes, and training datasets for question answering systems.\n\nText-Based Anomaly Detection: Embeddings can be employed to detect anomalous patterns or outliers in text data, helping to identify unusual or suspicious activities in applications such as cybersecurity and network monitoring.\n\nText-Based Personality Prediction: Embeddings can be utilized to predict personality traits from text data, enabling applications such as personalized recommendation systems and targeted advertising.\n\nText-Based Emotion Recognition: Embeddings can assist in recognizing emotions expressed in text, allowing for sentiment analysis at a finer-grained level by identifying specific emotions such as joy, anger, sadness, and fear.\n\nText-Based Event Forecasting: Embeddings can be applied to predict future events or trends based on historical text data, supporting forecasting tasks in domains such as finance, economics, and public health.\n\nText-Based Content Recommendation: Embeddings can aid in recommending relevant content to users based on their interests and preferences, improving user engagement and satisfaction in content consumption platforms.\n\nText-Based Automatic Essay Scoring: Embeddings can be used to automatically score essays and written assignments by analyzing the semantic content and quality of the text, facilitating assessment and feedback in educational settings.\n\nText-Based Sentiment Analysis in Customer Reviews: Embeddings can assist in analyzing sentiment in customer reviews to extract insights about product features, customer preferences, and overall satisfaction levels.\n\nText-Based Community Detection in Social Networks: Embeddings can be employed to detect communities or groups of users with similar interests or behaviors in social networks, enabling targeted marketing campaigns and community engagement strategies.\n\nText-Based News Summarization: Embeddings can be used to automatically summarize news articles by identifying key information and extracting the most important points, providing users with concise summaries of current events.\n\nText-Based Disease Surveillance: Embeddings can assist in monitoring and tracking disease outbreaks by analyzing textual data from sources such as news articles, social media posts, and medical reports to detect early warning signs and trends.\n\nText-Based Content Moderation: Embeddings can be applied to detect and filter inappropriate or harmful content in online platforms, supporting content moderation efforts to maintain a safe and respectful environment for users.\n\nText-Based Knowledge Graph Construction: Embeddings can aid in constructing knowledge graphs by encoding the semantic relationships between entities and concepts mentioned in text data, facilitating knowledge representation and reasoning.\n\nText-Based Document Similarity Measurement: Embeddings can be used to measure the similarity between pairs of documents or passages, enabling tasks such as document clustering, information retrieval, and plagiarism detection.\n\nText-Based Identity Resolution: Embeddings can assist in resolving identity references across different sources of text data, enabling entity resolution tasks in applications such as data integration and deduplication.\n\nText-Based Trend Analysis: Embeddings can be applied to analyze trends and patterns in textual data over time, helping organizations gain insights into evolving consumer preferences, market dynamics, and social trends.\n\nThese examples showcase the diverse range of applications for text embeddings across various domains, highlighting their versatility and utility in extracting valuable insights from textual data.","metadata":{}},{"cell_type":"code","source":"import time\nfrom sentence_transformers import SentenceTransformer\n\nt0 = time.time()\n\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\ndur = time.time() - t0\nprint(f\"time {dur:.0f}s\\n\")\nprint(embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T08:47:30.326923Z","iopub.execute_input":"2024-04-13T08:47:30.327353Z","iopub.status.idle":"2024-04-13T08:47:46.006591Z","shell.execute_reply.started":"2024-04-13T08:47:30.327301Z","shell.execute_reply":"2024-04-13T08:47:46.005067Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea7d2861e8c243bdbc87e79d256b1bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f656321e314c4312b5ac091fbf18c593"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c788f5eeff841cbbd71dc516ba070b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3140217ff9d84dce81a530373dbfd4bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e32d31e289b4e3db6a0211d339a4c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f51637ee8e7e4ce3915b6cea6accc0e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9919b449b9244ba6b3f9153e60bcb007"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f549fb7fdeb14882873322ee0a8755d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01c1869dd1bc42c094b33735d284c355"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd8601e1943d4cebbc31aca1427f281e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c96f28c094bb4764a5e897284deb7c93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b04553fb024c019026734e41a3d08d"}},"metadata":{}},{"name":"stdout","text":"time 7s\n\n[[ 6.76569566e-02  6.34959862e-02  4.87130918e-02  7.93049484e-02\n   3.74480672e-02  2.65282346e-03  3.93749624e-02 -7.09845359e-03\n   5.93614168e-02  3.15369852e-02  6.00980930e-02 -5.29052317e-02\n   4.06067558e-02 -2.59308554e-02  2.98428331e-02  1.12690055e-03\n   7.35148415e-02 -5.03818020e-02 -1.22386612e-01  2.37028580e-02\n   2.97265276e-02  4.24768068e-02  2.56337728e-02  1.99516723e-03\n  -5.69190681e-02 -2.71598399e-02 -3.29035260e-02  6.60248771e-02\n   1.19007170e-01 -4.58791256e-02 -7.26214051e-02 -3.25840078e-02\n   5.23413084e-02  4.50552925e-02  8.25302955e-03  3.67024206e-02\n  -1.39416149e-02  6.53918460e-02 -2.64272038e-02  2.06379103e-04\n  -1.36643406e-02 -3.62810567e-02 -1.95043534e-02 -2.89738216e-02\n   3.94270122e-02 -8.84091258e-02  2.62429030e-03  1.36713507e-02\n   4.83062826e-02 -3.11566256e-02 -1.17329165e-01 -5.11690453e-02\n  -8.85288492e-02 -2.18963306e-02  1.42986458e-02  4.44167443e-02\n  -1.34815611e-02  7.43392482e-02  2.66382769e-02 -1.98762957e-02\n   1.79191660e-02 -1.06052216e-02 -9.04262513e-02  2.13269256e-02\n   1.41204834e-01 -6.47182297e-03 -1.40380231e-03 -1.53609803e-02\n  -8.73571336e-02  7.22174719e-02  2.01403350e-02  4.25587595e-02\n  -3.49014141e-02  3.19477724e-04 -8.02970976e-02 -3.27473246e-02\n   2.85268407e-02 -5.13657928e-02  1.09389216e-01  8.19327161e-02\n  -9.84039977e-02 -9.34095755e-02 -1.51292216e-02  4.51248847e-02\n   4.94172275e-02 -2.51867305e-02  1.57077648e-02 -1.29290730e-01\n   5.31891966e-03  4.02341597e-03 -2.34572105e-02 -6.72982782e-02\n   2.92280130e-02 -2.60845087e-02  1.30625209e-02 -3.11662573e-02\n  -4.82713357e-02 -5.58859594e-02 -3.87504436e-02  1.20010890e-01\n  -1.03924228e-02  4.89705130e-02  5.53537309e-02  4.49358448e-02\n  -4.00980935e-03 -1.02959774e-01 -2.92968620e-02 -5.83402738e-02\n   2.70471927e-02 -2.20169090e-02 -7.22240880e-02 -4.13870178e-02\n  -1.93297956e-02  2.73325271e-03  2.76994164e-04 -9.67588872e-02\n  -1.00574709e-01 -1.41922664e-02 -8.07892010e-02  4.53925319e-02\n   2.45041437e-02  5.97614162e-02 -7.38184899e-02  1.19843846e-02\n  -6.63403720e-02 -7.69044980e-02  3.85157652e-02 -5.59362183e-33\n   2.80013923e-02 -5.60785122e-02 -4.86601703e-02  2.15569325e-02\n   6.01981133e-02 -4.81402650e-02 -3.50247175e-02  1.93313770e-02\n  -1.75151732e-02 -3.89209725e-02 -3.81063740e-03 -1.70287602e-02\n   2.82099899e-02  1.28291026e-02  4.71600518e-02  6.21029921e-02\n  -6.43588975e-02  1.29285559e-01 -1.31231081e-02  5.23069464e-02\n  -3.73680517e-02  2.89094299e-02 -1.68981086e-02 -2.37329844e-02\n  -3.33491378e-02 -5.16762324e-02  1.55356647e-02  2.08802428e-02\n  -1.25372112e-02  4.59578931e-02  3.72720174e-02  2.80567370e-02\n  -5.90005331e-02 -1.16988374e-02  4.92182821e-02  4.70329151e-02\n   7.35487714e-02 -3.70530374e-02  3.98461195e-03  1.06411977e-02\n  -1.61538817e-04 -5.27166054e-02  2.75927875e-02 -3.92921641e-02\n   8.44717771e-02  4.86860648e-02 -4.85870754e-03  1.79949086e-02\n  -4.28569838e-02  1.23375198e-02  6.39954116e-03  4.04823199e-02\n   1.48886982e-02 -1.53941168e-02  7.62948170e-02  2.37043407e-02\n   4.45237085e-02  5.08195721e-02 -2.31251190e-03 -1.88737344e-02\n  -1.23335375e-02  4.66001183e-02 -5.63437864e-02  6.29927665e-02\n  -3.15534994e-02  3.24912071e-02  2.34673321e-02 -6.55438751e-02\n   2.01708954e-02  2.57082433e-02 -1.23869134e-02 -8.36490653e-03\n  -6.64377883e-02  9.43073258e-02 -3.57092880e-02 -3.42482850e-02\n  -6.66358322e-03 -8.01530201e-03 -3.09711285e-02  4.33011949e-02\n  -8.21403693e-03 -1.50795028e-01  3.07691414e-02  4.00719307e-02\n  -3.79293188e-02  1.93216477e-03  4.00530845e-02 -8.77075270e-02\n  -3.68491150e-02  8.57963692e-03 -3.19251940e-02 -1.25257857e-02\n   7.35539719e-02  1.34737219e-03  2.05918737e-02  2.71098146e-33\n  -5.18576801e-02  5.78361303e-02 -9.18985009e-02  3.94421555e-02\n   1.05576500e-01 -1.96911618e-02  6.18401952e-02 -7.63465688e-02\n   2.40880512e-02  9.40048844e-02 -1.16535433e-01  3.71198170e-02\n   5.22425547e-02 -3.95857962e-03  5.72213754e-02  5.32850157e-03\n   1.24016911e-01  1.39022321e-02 -1.10249883e-02  3.56052890e-02\n  -3.30754407e-02  8.16573873e-02 -1.52003234e-02  6.05585389e-02\n  -6.01397902e-02  3.26102749e-02 -3.48296836e-02 -1.69881824e-02\n  -9.74907205e-02 -2.71484014e-02  1.74713123e-03 -7.68982247e-02\n  -4.31858003e-02 -1.89984553e-02 -2.91661043e-02  5.77488728e-02\n   2.41821650e-02 -1.16902348e-02 -6.21435083e-02  2.84351334e-02\n  -2.37512315e-04 -2.51783412e-02  4.39635990e-03  8.12839866e-02\n   3.64184193e-02 -6.04006313e-02 -3.65517363e-02 -7.93748125e-02\n  -5.08526154e-03  6.69698715e-02 -1.17784344e-01  3.23743261e-02\n  -4.71252576e-02 -1.34460144e-02 -9.48444903e-02  8.24956503e-03\n  -1.06748641e-02 -6.81881607e-02  1.11820106e-03  2.48020329e-02\n  -6.35889173e-02  2.84492839e-02 -2.61303857e-02  8.58111754e-02\n   1.14682205e-01 -5.35345487e-02 -5.63588366e-02  4.26009260e-02\n   1.09454412e-02  2.09579971e-02  1.00131169e-01  3.26051265e-02\n  -1.84208721e-01 -3.93208861e-02 -6.91454709e-02 -6.38104379e-02\n  -6.56385869e-02 -6.41250238e-03 -4.79612313e-02 -7.68132657e-02\n   2.95384545e-02 -2.29948312e-02  4.17036600e-02 -2.50047874e-02\n  -4.54502879e-03 -4.17136885e-02 -1.32289575e-02 -6.38357401e-02\n  -2.46473053e-03 -1.37337921e-02  1.68976635e-02 -6.30397573e-02\n   8.98880661e-02  4.18170914e-02 -1.85687467e-02 -1.80442186e-08\n  -1.67998131e-02 -3.21577974e-02  6.30383566e-02 -4.13092375e-02\n   4.44819555e-02  2.02468410e-03  6.29592687e-02 -5.17370738e-03\n  -1.00444471e-02 -3.05639785e-02  3.52672599e-02  5.58581278e-02\n  -4.67124358e-02  3.45103368e-02  3.29578333e-02  4.30115014e-02\n   2.94360705e-02 -3.03164423e-02 -1.71107724e-02  7.37484246e-02\n  -5.47909997e-02  2.77515016e-02  6.20168354e-03  1.58800352e-02\n   3.42978612e-02 -5.15751401e-03  2.35079769e-02  7.53135681e-02\n   1.92842670e-02  3.36197168e-02  5.09103425e-02  1.52497083e-01\n   1.64207891e-02  2.70528123e-02  3.75162177e-02  2.18553226e-02\n   5.66333421e-02 -3.95746343e-02  7.12313578e-02 -5.41377254e-02\n   1.03766436e-03  2.11853143e-02 -3.56308930e-02  1.09016888e-01\n   2.76531209e-03  3.13997120e-02  1.38414733e-03 -3.45738493e-02\n  -4.59277667e-02  2.88083311e-02  7.16901943e-03  4.84685041e-02\n   2.61017829e-02 -9.44073685e-03  2.82169189e-02  3.48724537e-02\n   3.69098857e-02 -8.58948939e-03 -3.53205651e-02 -2.47856639e-02\n  -1.91921555e-02  3.80707867e-02  5.99653199e-02 -4.22287062e-02]\n [ 8.64386111e-02  1.02762699e-01  5.39458729e-03  2.04443466e-03\n  -9.96334013e-03  2.53855083e-02  4.92875464e-02 -3.06265801e-02\n   6.87254891e-02  1.01365987e-02  7.75398165e-02 -9.00807679e-02\n   6.10617734e-03 -5.69899119e-02  1.41714616e-02  2.80491710e-02\n  -8.68465155e-02  7.64399171e-02 -1.03491336e-01 -6.77438006e-02\n   6.99946806e-02  8.44251066e-02 -7.24913785e-03  1.04770595e-02\n   1.34020811e-02  6.77576959e-02 -9.42086577e-02 -3.71689834e-02\n   5.22617698e-02 -3.10853180e-02 -9.63407010e-02  1.57716833e-02\n   2.57866811e-02  7.85244927e-02  7.89949372e-02  1.91516262e-02\n   1.64356492e-02  3.10087367e-03  3.81311700e-02  2.37090848e-02\n   1.05389515e-02 -4.40645069e-02  4.41738665e-02 -2.58728135e-02\n   6.15378693e-02 -4.05427292e-02 -8.64139944e-02  3.19722705e-02\n  -8.90663476e-04 -2.44437214e-02 -9.19721127e-02  2.33939253e-02\n  -8.30293521e-02  4.41510268e-02 -2.49692891e-02  6.23020269e-02\n  -1.30349211e-03  7.51395598e-02  2.46384796e-02 -6.47244230e-02\n  -1.17727771e-01  3.83392423e-02 -9.11767185e-02  6.35445938e-02\n   7.62739182e-02 -8.80241245e-02  9.54559632e-03 -4.69717830e-02\n  -8.41740966e-02  3.88823599e-02 -1.14393547e-01  6.28857967e-03\n  -3.49361710e-02  2.39750296e-02 -3.31317000e-02 -1.57243945e-02\n  -3.78955677e-02 -8.81249085e-03  7.06119016e-02  3.28066275e-02\n   2.03673425e-03 -1.12278998e-01  6.79719448e-03  1.22765647e-02\n   3.35303321e-02 -1.36200571e-02 -2.25489810e-02 -2.25229040e-02\n  -2.03194823e-02  5.04297540e-02 -7.48652667e-02 -8.22822601e-02\n   7.65962228e-02  4.93392348e-02 -3.75553407e-02  1.44634405e-02\n  -5.72457798e-02 -1.79954283e-02  1.09697953e-01  1.19462796e-01\n   8.09248188e-04  6.17057420e-02  3.26322503e-02 -1.30780071e-01\n  -1.48636684e-01 -6.16233014e-02  4.33886386e-02  2.67129298e-02\n   1.39786154e-02 -3.94002385e-02 -2.52711903e-02  3.87739646e-03\n   3.58664319e-02 -6.15420491e-02  3.76660451e-02  2.67565399e-02\n  -3.82659286e-02 -3.54793631e-02 -2.39227712e-02  8.67977217e-02\n  -1.84062999e-02  7.71039575e-02  1.39865256e-03  7.00383335e-02\n  -4.77878004e-02 -7.89820105e-02  5.10814451e-02 -2.99868333e-33\n  -3.91646177e-02 -2.56211404e-03  1.65210329e-02  9.48937051e-03\n  -5.66219538e-02  6.57783151e-02 -4.77002673e-02  1.11662047e-02\n  -5.73558174e-02 -9.16259829e-03 -2.17521153e-02 -5.59531786e-02\n  -1.11423135e-02  9.32793245e-02  1.66765098e-02 -1.36723993e-02\n   4.34388630e-02  1.87240262e-03  7.29945628e-03  5.16332127e-02\n   4.80608903e-02  1.35341436e-01 -1.71739049e-02 -1.29698124e-02\n  -7.50110000e-02  2.61107795e-02  2.69802194e-02  7.83063821e-04\n  -4.87270132e-02  1.17842713e-02 -4.59580310e-02 -4.83213700e-02\n  -1.95671003e-02  1.93889029e-02  1.98807232e-02  1.67432334e-02\n   9.87801179e-02 -2.74087936e-02  2.34809201e-02  3.70230433e-03\n  -6.14514537e-02 -1.21228490e-03 -9.50471312e-03  9.25152563e-03\n   2.38444265e-02  8.61232206e-02  2.26789918e-02  5.45154035e-04\n   3.47129516e-02  6.25464460e-03 -6.92777336e-03  3.92400697e-02\n   1.15674781e-02  3.26279774e-02  6.22155666e-02  2.76114289e-02\n   1.86883509e-02  3.55805941e-02  4.11795676e-02  1.54781677e-02\n   4.22691405e-02  3.82248126e-02  1.00313528e-02 -2.83245910e-02\n   4.47052419e-02 -4.10458744e-02 -4.50550858e-03 -5.44734299e-02\n   2.62320973e-02  1.79862715e-02 -1.23118773e-01 -4.66951914e-02\n  -1.35913165e-02  6.46710619e-02  3.57348961e-03 -1.22233871e-02\n  -1.79382600e-02 -2.55502146e-02  2.37224232e-02  4.08663042e-03\n  -6.51476085e-02  4.43651527e-02  4.68595922e-02 -3.25175039e-02\n   4.02270118e-03 -3.97605542e-03  1.11939618e-02 -9.95597914e-02\n   3.33168507e-02  8.01060870e-02  9.42692533e-02 -6.38293922e-02\n   3.23151611e-02 -5.13553545e-02 -7.49880821e-03  5.30048586e-34\n  -4.13195007e-02  9.49646831e-02 -1.06401436e-01  4.96590622e-02\n  -3.41913328e-02 -3.16745825e-02 -1.71556007e-02  1.70102390e-03\n   5.79757802e-02 -1.21777714e-03 -1.68536324e-02 -5.16912527e-02\n   5.52998744e-02 -3.42647582e-02  3.08179501e-02 -3.10480949e-02\n   9.27532986e-02  3.72663699e-02 -2.37398222e-02  4.45893705e-02\n   1.46153271e-02  1.16239369e-01 -5.00112474e-02  3.88716906e-02\n   4.24742280e-03  2.56976597e-02  3.27243805e-02  4.29907665e-02\n  -1.36144478e-02  2.56122164e-02  1.06262565e-02 -8.46863613e-02\n  -9.52982530e-02  1.08399883e-01 -7.51600116e-02 -1.37773659e-02\n   6.37338310e-02 -4.49670665e-03 -3.25321853e-02  6.23614080e-02\n   3.48053053e-02 -3.54922563e-02 -2.00222619e-02  3.66608314e-02\n  -2.48837173e-02  1.01818591e-02 -7.01233223e-02 -4.31950875e-02\n   2.95332801e-02 -2.94909900e-04 -3.45386527e-02  1.46675892e-02\n  -9.83970016e-02 -4.70488183e-02 -8.85493960e-03 -8.89914036e-02\n   3.50996181e-02 -1.29601970e-01 -4.98866327e-02 -6.12047389e-02\n  -5.97797930e-02  9.46321152e-03  4.91217934e-02 -7.75026530e-02\n   8.09726864e-02 -4.79257368e-02  2.34378339e-03  7.57031813e-02\n  -2.40175240e-02 -1.52545795e-02  4.86738607e-02 -3.85968685e-02\n  -7.04831779e-02 -1.20348725e-02 -3.88790853e-02 -7.76017234e-02\n  -1.07244272e-02  1.04188649e-02 -2.13753246e-02 -9.17386487e-02\n  -1.11344690e-02 -2.96066180e-02  2.46458184e-02  4.65709390e-03\n  -1.63450222e-02 -3.95219699e-02  7.73373395e-02 -2.84732860e-02\n  -3.69937881e-03  8.27665329e-02 -1.10408906e-02  3.13983411e-02\n   5.35094179e-02  5.75145856e-02 -3.17622013e-02 -1.52911284e-08\n  -7.99661353e-02 -4.76796776e-02 -8.59788731e-02  5.69616668e-02\n  -4.08866592e-02  2.23832391e-02 -4.64446656e-03 -3.80130634e-02\n  -3.10671106e-02 -1.07277865e-02  1.97698381e-02  7.77000468e-03\n  -6.09470718e-03 -3.86376232e-02  2.80272197e-02  6.78138360e-02\n  -2.35351454e-02  3.21747363e-02  8.02537333e-03 -2.39107292e-02\n  -1.22001441e-03  3.14599164e-02 -5.24924137e-02 -8.06818157e-03\n   3.14773805e-03  5.11496589e-02 -4.44104634e-02  6.36013448e-02\n   3.85084376e-02  3.30433138e-02 -4.18725563e-03  4.95593064e-02\n  -5.69605529e-02 -6.49713492e-03 -2.49793064e-02 -1.60867386e-02\n   6.62289634e-02 -2.06310600e-02  1.08045787e-01  1.68547314e-02\n   1.43812746e-02 -1.32127274e-02 -1.29387423e-01  6.95216581e-02\n  -5.55773415e-02 -6.75413311e-02 -5.45818591e-03 -6.13592193e-03\n   3.90840583e-02 -6.28779829e-02  3.74063440e-02 -1.16570340e-02\n   1.29149919e-02 -5.52495159e-02  5.16075864e-02 -4.30840766e-03\n   5.80247827e-02  1.86944846e-02  2.27810331e-02  3.21665220e-02\n   5.37979119e-02  7.02848807e-02  7.49312043e-02 -8.41774866e-02]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\nt0 = time.time()\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\n# p (float) – the exponent value in the norm formulation. Default: 2\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\ndur = time.time() - t0\nprint(f\"time {dur:.0f}s\\n\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T09:01:32.930507Z","iopub.execute_input":"2024-04-13T09:01:32.930925Z","iopub.status.idle":"2024-04-13T09:01:33.503744Z","shell.execute_reply.started":"2024-04-13T09:01:32.930896Z","shell.execute_reply":"2024-04-13T09:01:33.502412Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"time 1s\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Sentence embeddings:\")\nprint(sentence_embeddings[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-13T09:01:37.173361Z","iopub.execute_input":"2024-04-13T09:01:37.173779Z","iopub.status.idle":"2024-04-13T09:01:37.185661Z","shell.execute_reply.started":"2024-04-13T09:01:37.173748Z","shell.execute_reply":"2024-04-13T09:01:37.184494Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Sentence embeddings:\ntensor([ 6.7657e-02,  6.3496e-02,  4.8713e-02,  7.9305e-02,  3.7448e-02,\n         2.6528e-03,  3.9375e-02, -7.0985e-03,  5.9361e-02,  3.1537e-02,\n         6.0098e-02, -5.2905e-02,  4.0607e-02, -2.5931e-02,  2.9843e-02,\n         1.1269e-03,  7.3515e-02, -5.0382e-02, -1.2239e-01,  2.3703e-02,\n         2.9727e-02,  4.2477e-02,  2.5634e-02,  1.9952e-03, -5.6919e-02,\n        -2.7160e-02, -3.2904e-02,  6.6025e-02,  1.1901e-01, -4.5879e-02,\n        -7.2621e-02, -3.2584e-02,  5.2341e-02,  4.5055e-02,  8.2530e-03,\n         3.6702e-02, -1.3942e-02,  6.5392e-02, -2.6427e-02,  2.0638e-04,\n        -1.3664e-02, -3.6281e-02, -1.9504e-02, -2.8974e-02,  3.9427e-02,\n        -8.8409e-02,  2.6243e-03,  1.3671e-02,  4.8306e-02, -3.1157e-02,\n        -1.1733e-01, -5.1169e-02, -8.8529e-02, -2.1896e-02,  1.4299e-02,\n         4.4417e-02, -1.3482e-02,  7.4339e-02,  2.6638e-02, -1.9876e-02,\n         1.7919e-02, -1.0605e-02, -9.0426e-02,  2.1327e-02,  1.4120e-01,\n        -6.4718e-03, -1.4038e-03, -1.5361e-02, -8.7357e-02,  7.2217e-02,\n         2.0140e-02,  4.2559e-02, -3.4901e-02,  3.1948e-04, -8.0297e-02,\n        -3.2747e-02,  2.8527e-02, -5.1366e-02,  1.0939e-01,  8.1933e-02,\n        -9.8404e-02, -9.3410e-02, -1.5129e-02,  4.5125e-02,  4.9417e-02,\n        -2.5187e-02,  1.5708e-02, -1.2929e-01,  5.3189e-03,  4.0234e-03,\n        -2.3457e-02, -6.7298e-02,  2.9228e-02, -2.6085e-02,  1.3063e-02,\n        -3.1166e-02, -4.8271e-02, -5.5886e-02, -3.8750e-02,  1.2001e-01,\n        -1.0392e-02,  4.8971e-02,  5.5354e-02,  4.4936e-02, -4.0098e-03,\n        -1.0296e-01, -2.9297e-02, -5.8340e-02,  2.7047e-02, -2.2017e-02,\n        -7.2224e-02, -4.1387e-02, -1.9330e-02,  2.7333e-03,  2.7699e-04,\n        -9.6759e-02, -1.0057e-01, -1.4192e-02, -8.0789e-02,  4.5393e-02,\n         2.4504e-02,  5.9761e-02, -7.3818e-02,  1.1984e-02, -6.6340e-02,\n        -7.6904e-02,  3.8516e-02, -5.5936e-33,  2.8001e-02, -5.6079e-02,\n        -4.8660e-02,  2.1557e-02,  6.0198e-02, -4.8140e-02, -3.5025e-02,\n         1.9331e-02, -1.7515e-02, -3.8921e-02, -3.8106e-03, -1.7029e-02,\n         2.8210e-02,  1.2829e-02,  4.7160e-02,  6.2103e-02, -6.4359e-02,\n         1.2929e-01, -1.3123e-02,  5.2307e-02, -3.7368e-02,  2.8909e-02,\n        -1.6898e-02, -2.3733e-02, -3.3349e-02, -5.1676e-02,  1.5536e-02,\n         2.0880e-02, -1.2537e-02,  4.5958e-02,  3.7272e-02,  2.8057e-02,\n        -5.9001e-02, -1.1699e-02,  4.9218e-02,  4.7033e-02,  7.3549e-02,\n        -3.7053e-02,  3.9846e-03,  1.0641e-02, -1.6154e-04, -5.2717e-02,\n         2.7593e-02, -3.9292e-02,  8.4472e-02,  4.8686e-02, -4.8587e-03,\n         1.7995e-02, -4.2857e-02,  1.2338e-02,  6.3995e-03,  4.0482e-02,\n         1.4889e-02, -1.5394e-02,  7.6295e-02,  2.3704e-02,  4.4524e-02,\n         5.0820e-02, -2.3125e-03, -1.8874e-02, -1.2334e-02,  4.6600e-02,\n        -5.6344e-02,  6.2993e-02, -3.1553e-02,  3.2491e-02,  2.3467e-02,\n        -6.5544e-02,  2.0171e-02,  2.5708e-02, -1.2387e-02, -8.3649e-03,\n        -6.6438e-02,  9.4307e-02, -3.5709e-02, -3.4248e-02, -6.6636e-03,\n        -8.0153e-03, -3.0971e-02,  4.3301e-02, -8.2140e-03, -1.5080e-01,\n         3.0769e-02,  4.0072e-02, -3.7929e-02,  1.9322e-03,  4.0053e-02,\n        -8.7708e-02, -3.6849e-02,  8.5796e-03, -3.1925e-02, -1.2526e-02,\n         7.3554e-02,  1.3474e-03,  2.0592e-02,  2.7110e-33, -5.1858e-02,\n         5.7836e-02, -9.1899e-02,  3.9442e-02,  1.0558e-01, -1.9691e-02,\n         6.1840e-02, -7.6347e-02,  2.4088e-02,  9.4005e-02, -1.1654e-01,\n         3.7120e-02,  5.2243e-02, -3.9586e-03,  5.7221e-02,  5.3285e-03,\n         1.2402e-01,  1.3902e-02, -1.1025e-02,  3.5605e-02, -3.3075e-02,\n         8.1657e-02, -1.5200e-02,  6.0559e-02, -6.0140e-02,  3.2610e-02,\n        -3.4830e-02, -1.6988e-02, -9.7491e-02, -2.7148e-02,  1.7471e-03,\n        -7.6898e-02, -4.3186e-02, -1.8998e-02, -2.9166e-02,  5.7749e-02,\n         2.4182e-02, -1.1690e-02, -6.2144e-02,  2.8435e-02, -2.3751e-04,\n        -2.5178e-02,  4.3964e-03,  8.1284e-02,  3.6418e-02, -6.0401e-02,\n        -3.6552e-02, -7.9375e-02, -5.0853e-03,  6.6970e-02, -1.1778e-01,\n         3.2374e-02, -4.7125e-02, -1.3446e-02, -9.4844e-02,  8.2496e-03,\n        -1.0675e-02, -6.8188e-02,  1.1182e-03,  2.4802e-02, -6.3589e-02,\n         2.8449e-02, -2.6130e-02,  8.5811e-02,  1.1468e-01, -5.3535e-02,\n        -5.6359e-02,  4.2601e-02,  1.0945e-02,  2.0958e-02,  1.0013e-01,\n         3.2605e-02, -1.8421e-01, -3.9321e-02, -6.9145e-02, -6.3810e-02,\n        -6.5639e-02, -6.4125e-03, -4.7961e-02, -7.6813e-02,  2.9538e-02,\n        -2.2995e-02,  4.1704e-02, -2.5005e-02, -4.5450e-03, -4.1714e-02,\n        -1.3229e-02, -6.3836e-02, -2.4647e-03, -1.3734e-02,  1.6898e-02,\n        -6.3040e-02,  8.9888e-02,  4.1817e-02, -1.8569e-02, -1.8044e-08,\n        -1.6800e-02, -3.2158e-02,  6.3038e-02, -4.1309e-02,  4.4482e-02,\n         2.0247e-03,  6.2959e-02, -5.1737e-03, -1.0044e-02, -3.0564e-02,\n         3.5267e-02,  5.5858e-02, -4.6712e-02,  3.4510e-02,  3.2958e-02,\n         4.3012e-02,  2.9436e-02, -3.0316e-02, -1.7111e-02,  7.3748e-02,\n        -5.4791e-02,  2.7752e-02,  6.2017e-03,  1.5880e-02,  3.4298e-02,\n        -5.1575e-03,  2.3508e-02,  7.5314e-02,  1.9284e-02,  3.3620e-02,\n         5.0910e-02,  1.5250e-01,  1.6421e-02,  2.7053e-02,  3.7516e-02,\n         2.1855e-02,  5.6633e-02, -3.9575e-02,  7.1231e-02, -5.4138e-02,\n         1.0377e-03,  2.1185e-02, -3.5631e-02,  1.0902e-01,  2.7653e-03,\n         3.1400e-02,  1.3841e-03, -3.4574e-02, -4.5928e-02,  2.8808e-02,\n         7.1690e-03,  4.8469e-02,  2.6102e-02, -9.4407e-03,  2.8217e-02,\n         3.4872e-02,  3.6910e-02, -8.5895e-03, -3.5321e-02, -2.4786e-02,\n        -1.9192e-02,  3.8071e-02,  5.9965e-02, -4.2229e-02])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Although the use of huggingface package is quicker, sentence transformer package is easier because there are many examples using the package. We can use the examples to understand the embeddings easily.","metadata":{}},{"cell_type":"markdown","source":"**Input Sequence Length**\nTransformer models like BERT / RoBERTa / DistilBERT etc. the runtime and the memory requirement grows quadratic with the input length. This limits transformers to inputs of certain lengths. A common value for BERT & Co. are 512 word pieces, which corresponds to about 300-400 words (for English). \nLonger texts than this are truncated to the first x word pieces.\n\nNote: You cannot increase the length higher than what is maximally supported by the respective transformer model. Also note that if a model was trained on short texts, the representations for long texts might not be that good.\n\nBy default, the provided methods use a limit of 128 word pieces, longer inputs will be truncated. You can get and set the maximal sequence length like this:","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\nprint(\"Max Sequence Length:\", model.max_seq_length)\n\n# Change the length to 200\nmodel.max_seq_length = 200\n\nprint(\"Max Sequence Length:\", model.max_seq_length)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T10:40:19.377360Z","iopub.execute_input":"2024-04-13T10:40:19.378026Z","iopub.status.idle":"2024-04-13T10:40:34.428071Z","shell.execute_reply.started":"2024-04-13T10:40:19.377972Z","shell.execute_reply":"2024-04-13T10:40:34.426381Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4828f00ef54a4c81a7768c47deae2b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5b06863d2c34c71b0e3354458f34b0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d02c345f34ac46009578087fff395a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"671c66df244947e08f65db3b131da103"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78fb6b1dda7341ff9f69ad34eaafa3e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53bd773cef5c4a1eb26f1c5147a9fe41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd3f4f29978647f9834e6ad1f5ec9047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0fb98e7026849dcb948841cba67c485"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"429f05f5c7e94216926ddf7fba078e36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0955d20436e6445d85e578e4201fb840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4fecefa1014792aa4482c251b3761c"}},"metadata":{}},{"name":"stdout","text":"Max Sequence Length: 256\nMax Sequence Length: 200\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Storing & Loading Embeddings**\nThe easiest method is to use pickle to store pre-computed embeddings on disc and to load it from disc. This can especially be useful if you need to encode large set of sentences.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport pickle\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nsentences = [\n    \"This framework generates embeddings for each input sentence\",\n    \"Sentences are passed as a list of string.\",\n    \"The quick brown fox jumps over the lazy dog.\",\n]\n\n\nembeddings = model.encode(sentences)\n\n# Store sentences & embeddings on disc\nwith open(\"embeddings.pkl\", \"wb\") as fOut:\n    pickle.dump({\"sentences\": sentences, \"embeddings\": embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n\n# Load sentences & embeddings from disc\nwith open(\"embeddings.pkl\", \"rb\") as fIn:\n    stored_data = pickle.load(fIn)\n    stored_sentences = stored_data[\"sentences\"]\n    stored_embeddings = stored_data[\"embeddings\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T10:41:44.968396Z","iopub.execute_input":"2024-04-13T10:41:44.969044Z","iopub.status.idle":"2024-04-13T10:41:46.170638Z","shell.execute_reply.started":"2024-04-13T10:41:44.969012Z","shell.execute_reply":"2024-04-13T10:41:46.169414Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe25153b7ac94e2e9b2b879e0313a71d"}},"metadata":{}}]},{"cell_type":"markdown","source":"Sentence Embeddings with Transformers\n\nMost of our pre-trained models are based on Huggingface.co/Transformers and are also hosted in the models repository from Huggingface. It is possible to use our sentence embeddings models without installing sentence-transformers:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\n\n# Sentences we want sentence embeddings for\nsentences = [\n    \"This framework generates embeddings for each input sentence\",\n    \"Sentences are passed as a list of string.\",\n    \"The quick brown fox jumps over the lazy dog.\",\n]\n\n# Load AutoModel from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Tokenize sentences\nencoded_input = tokenizer(\n    sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n)\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-13T10:43:32.662426Z","iopub.execute_input":"2024-04-13T10:43:32.663765Z","iopub.status.idle":"2024-04-13T10:43:33.012301Z","shell.execute_reply.started":"2024-04-13T10:43:32.663706Z","shell.execute_reply":"2024-04-13T10:43:33.011187Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Semantic Similarity Search\nOnce you have sentence embeddings computed, you usually want to compare them to each other. Here, I show you how you can compute the cosine similarity between embeddings, for example, to measure the semantic similarity of two texts.\n\nWe pass the convert_to_tensor=True parameter to the encode function. This will return a pytorch tensor containing our embeddings. We can then call util.cos_sim(A, B) which computes the cosine similarity between all vectors in A and all vectors in B.\n\nIt returns in the above example a 3x3 matrix with the respective cosine similarity scores for all possible pairs between embeddings1 and embeddings2.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Two lists of sentences\nsentences1 = [\n    \"The cat sits outside\",\n    \"A man is playing guitar\",\n    \"The new movie is awesome\",\n]\n\nsentences2 = [\n    \"The dog plays in the garden\",\n    \"A woman watches TV\",\n    \"The new movie is so great\",\n]\n\n# Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\n\n# Compute cosine-similarities\ncosine_scores = util.cos_sim(embeddings1, embeddings2)\n\n# Output the pairs with their score\nfor i in range(len(sentences1)):\n    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(\n        sentences1[i], sentences2[i], cosine_scores[i][i]\n    ))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T09:43:29.700559Z","iopub.execute_input":"2024-04-13T09:43:29.701123Z","iopub.status.idle":"2024-04-13T09:43:30.962339Z","shell.execute_reply.started":"2024-04-13T09:43:29.701083Z","shell.execute_reply":"2024-04-13T09:43:30.961004Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78b24278216d4e1ab3c1e91d6353fe50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6694ac1f5cda4e248d9d6acf5db62e1c"}},"metadata":{}},{"name":"stdout","text":"The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2838\nA man is playing guitar \t\t A woman watches TV \t\t Score: -0.0327\nThe new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You can use this function also to find out the pairs with the highest cosine similarity scores. we use a brute-force approach to find the highest scoring pairs, which has a quadratic complexity. For long lists of sentences, this might be infeasible. If you want find the highest scoring pairs in a long list of sentences, have a look at Paraphrase Mining.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Single list of sentences\nsentences = [\n    \"The cat sits outside\",\n    \"A man is playing guitar\",\n    \"I love pasta\",\n    \"The new movie is awesome\",\n    \"The cat plays in the garden\",\n    \"A woman watches TV\",\n    \"The new movie is so great\",\n    \"Do you like pizza?\",\n]\n\n# Compute embeddings\nembeddings = model.encode(sentences, convert_to_tensor=True)\n\n# Compute cosine-similarities for each sentence with each other sentence\ncosine_scores = util.cos_sim(embeddings, embeddings)\n\n# Find the pairs with the highest cosine similarity scores\npairs = []\nfor i in range(cosine_scores.shape[0]):\n    for j in range(cosine_scores.shape[1]):\n        pairs.append({\"index\": [i, j], \"score\": cosine_scores[i][j]})\n\n# Sort scores in decreasing order\npairs = sorted(pairs, key=lambda x: x[\"score\"], reverse=True)\n\nfor pair in pairs[0:10]:\n    i, j = pair[\"index\"]\n    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(\n        sentences[i], sentences[j], pair[\"score\"]\n    ))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T10:46:06.167326Z","iopub.execute_input":"2024-04-13T10:46:06.167848Z","iopub.status.idle":"2024-04-13T10:46:07.117523Z","shell.execute_reply.started":"2024-04-13T10:46:06.167816Z","shell.execute_reply":"2024-04-13T10:46:07.116005Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f148a020be704c6489baf9dc84ec87b4"}},"metadata":{}},{"name":"stdout","text":"The cat plays in the garden \t\t The cat plays in the garden \t\t Score: 1.0000\nThe new movie is so great \t\t The new movie is so great \t\t Score: 1.0000\nI love pasta \t\t I love pasta \t\t Score: 1.0000\nDo you like pizza? \t\t Do you like pizza? \t\t Score: 1.0000\nThe cat sits outside \t\t The cat sits outside \t\t Score: 1.0000\nA man is playing guitar \t\t A man is playing guitar \t\t Score: 1.0000\nThe new movie is awesome \t\t The new movie is awesome \t\t Score: 1.0000\nA woman watches TV \t\t A woman watches TV \t\t Score: 1.0000\nThe new movie is so great \t\t The new movie is awesome \t\t Score: 0.8939\nThe new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Embedding Quantization**\nEmbeddings may be challenging to scale up, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in float32, i.e., they require 4 bytes per dimension. To perform retrieval over 50 million vectors, you would therefore need around 200GB of memory. This tends to require complex and costly solutions at scale.\n\nHowever, there is a new approach to counter this problem; it entails reducing the size of each of the individual values in the embedding: Quantization. Experiments on quantization have shown that we can maintain a large amount of performance while significantly speeding up computation and saving on memory, storage, and costs.\n\n### Binary Quantization\nBinary quantization refers to the conversion of the float32 values in an embedding to 1-bit values, resulting in a 32x reduction in memory and storage usage. To quantize float32 embeddings to binary, we simply threshold normalized embeddings at 0: if the value is larger than 0, we make it 1, otherwise we convert it to 0. We can use the Hamming Distance to efficiently perform retrieval with these binary embeddings. This is simply the number of positions at which the bits of two binary embeddings differ. The lower the Hamming Distance, the closer the embeddings, and thus the more relevant the document. A huge advantage of the Hamming Distance is that it can be easily calculated with 2 CPU cycles, allowing for blazingly fast performance.\n\n* https://www.sbert.net/examples/applications/embedding-quantization/README.html","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2a. Encode some text using \"binary\" quantization\nbinary_embeddings = model.encode(\n    ['This is an example sentence', 'Each sentence is converted'],\n    precision=\"binary\",\n)\n\n# 2b. or, encode some text without quantization & apply quantization afterwards\nembeddings = model.encode(['This is an example sentence', 'Each sentence is converted'])\nbinary_embeddings = quantize_embeddings(embeddings, precision=\"binary\")\n\nprint(binary_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T10:54:10.334803Z","iopub.execute_input":"2024-04-13T10:54:10.335312Z","iopub.status.idle":"2024-04-13T10:54:12.623518Z","shell.execute_reply.started":"2024-04-13T10:54:10.335273Z","shell.execute_reply":"2024-04-13T10:54:12.622228Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"206069e9870d499e9191ea3d2aaf83d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f089cf14993942df997287f771894d59"}},"metadata":{}},{"name":"stdout","text":"[[  34  116 -108  -19  -86   34  -82 -100   25    9  -61  -79   -3  -11\n    29  -19  -83   69   84   72 -107  -41   55   19  -16 -109  -46  -41\n    23   -2  118  124   71   48  -54  -33   98  -64  109   95    4 -113\n   -71  111  118   99  -96   97   68   14  113   87  -13   44   26  -41\n   -96 -101  -49  108  -96   48  -68   29  113   22 -127  102  -34   67\n   -63   12 -103   57   16  -88   84  -74   34  -99  111   57 -116  112\n    -6   34  -82  -30  -98   11  111  -24  -67 -127   10  -71   10   91\n   -67   24   20   48    3  122  -62  117   31  -28   19  -81  -76 -111\n    29   61   92 -117   -9  -45   51   49  -63  -30   20  -49   -8 -101\n  -118  -66]\n [  51  -16 -128   -1   42   34  -18  -34 -104   81  -29   59   -3  -92\n    20  -87  -83  -11   68   73   28  -89   23   55  -16  -45  -42  -41\n   119  124  -12  -36   71  120  -37  -99  104 -128  -19   93 -100   71\n   -71  101  102   99   52  105   69   10  113  -41  -14   14  -98   95\n    42  -14  -22  -56    3   40  -71  -55  -71   23  -96   68  -42   90\n    64   14 -107   61 -124  -20  -12  -74  -62 -103   43  122 -100  104\n   -94   36  -83  -18   26 -110  111  -22  -67  -93   59 -103  104   73\n   121   29   84 -112    2   -5  -78  -41   29  110  -74  -65  -44 -117\n    15  109   24   11   -2  -42   35   49  -24   18   84 -117   56  -21\n  -110   56]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Max Sequence Length:\", model.max_seq_length)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:00:03.726050Z","iopub.execute_input":"2024-04-13T11:00:03.727107Z","iopub.status.idle":"2024-04-13T11:00:03.735046Z","shell.execute_reply.started":"2024-04-13T11:00:03.727065Z","shell.execute_reply":"2024-04-13T11:00:03.733209Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Max Sequence Length: 512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Read more about Quatization as it is more about optimization and use with lots of documents\n* https://www.sbert.net/examples/applications/embedding-quantization/README.html","metadata":{}},{"cell_type":"markdown","source":"**Semantic Search**\n\nSemantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines which only find documents based on lexical matches, semantic search can also find synonyms.\n\nBackground\nThe idea behind semantic search is to embed all entries in your corpus, whether they be sentences, paragraphs, or documents, into a vector space.\n\nAt search time, the query is embedded into the same vector space and the closest embeddings from your corpus are found. These entries should have a high semantic overlap with the query.\n\nSymmetric vs. Asymmetric Semantic Search\nA critical distinction for your setup is symmetric vs. asymmetric semantic search:\n\nFor symmetric semantic search your query and the entries in your corpus are of about the same length and have the same amount of content. An example would be searching for similar questions: Your query could for example be “How to learn Python online?” and you want to find an entry like “How to learn Python on the web?”. For symmetric tasks, you could potentially flip the query and the entries in your corpus.\n\nFor asymmetric semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query. An example would be a query like “What is Python” and you want to find the paragraph “Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense.\n\nIt is critical that you choose the right model for your type of task.\n\n* https://www.sbert.net/examples/applications/semantic-search/README.html\n\n#### Python\nFor small corpora (up to about 1 million entries) we can compute the cosine-similarity between the query and all entries in the corpus.\n\nIn the following example, we define a small corpus with few example sentences and compute the embeddings for the corpus as well as for our query.\n\nWe then use the util.cos_sim() function to compute the cosine similarity between the query and all corpus entries.\n\nFor large corpora, sorting all scores would take too much time. Hence, we use torch.topk to only get the top k entries.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThis is a simple application for sentence embeddings: semantic search\n\nWe have a corpus with various sentences. Then, for a given query sentence,\nwe want to find the most similar sentence in this corpus.\n\nThis script outputs for various queries the top 5 most similar sentences in the corpus.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Corpus with example sentences\ncorpus = [\n    \"A man is eating food.\",\n    \"A man is eating a piece of bread.\",\n    \"The girl is carrying a baby.\",\n    \"A man is riding a horse.\",\n    \"A woman is playing violin.\",\n    \"Two men pushed carts through the woods.\",\n    \"A man is riding a white horse on an enclosed ground.\",\n    \"A monkey is playing drums.\",\n    \"A cheetah is running behind its prey.\",\n]\ncorpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n\n# Query sentences:\nqueries = [\n    \"A man is eating pasta.\",\n    \"Someone in a gorilla costume is playing a set of drums.\",\n    \"A cheetah chases prey on across a field.\",\n]\n\n\n# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\ntop_k = min(5, len(corpus))\nfor query in queries:\n    query_embedding = embedder.encode(query, convert_to_tensor=True)\n\n    # We use cosine-similarity and torch.topk to find the highest 5 scores\n    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n    top_results = torch.topk(cos_scores, k=top_k)\n\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n\n    for score, idx in zip(top_results[0], top_results[1]):\n        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n\n    \"\"\"\n    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\n    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n    hits = hits[0]      #Get the hits for the first query\n    for hit in hits:\n        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:04:37.848900Z","iopub.execute_input":"2024-04-13T11:04:37.849455Z","iopub.status.idle":"2024-04-13T11:04:39.131754Z","shell.execute_reply.started":"2024-04-13T11:04:37.849422Z","shell.execute_reply":"2024-04-13T11:04:39.130076Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a5521ef94714f0abb7db3217dd37ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddecc1b284e94e2fa7d2e45b5213ce2d"}},"metadata":{}},{"name":"stdout","text":"\n\n======================\n\n\nQuery: A man is eating pasta.\n\nTop 5 most similar sentences in corpus:\nA man is eating food. (Score: 0.7035)\nA man is eating a piece of bread. (Score: 0.5272)\nA man is riding a horse. (Score: 0.1889)\nA man is riding a white horse on an enclosed ground. (Score: 0.1047)\nA cheetah is running behind its prey. (Score: 0.0980)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e0572001c3d4f7c8448c56d2aeeef26"}},"metadata":{}},{"name":"stdout","text":"\n\n======================\n\n\nQuery: Someone in a gorilla costume is playing a set of drums.\n\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.6433)\nA woman is playing violin. (Score: 0.2564)\nA man is riding a horse. (Score: 0.1389)\nA man is riding a white horse on an enclosed ground. (Score: 0.1191)\nA cheetah is running behind its prey. (Score: 0.1080)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32198cb1d5c74c6db9b0d6cb51081424"}},"metadata":{}},{"name":"stdout","text":"\n\n======================\n\n\nQuery: A cheetah chases prey on across a field.\n\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.8253)\nA man is eating food. (Score: 0.1399)\nA monkey is playing drums. (Score: 0.1292)\nA man is riding a white horse on an enclosed ground. (Score: 0.1097)\nA man is riding a horse. (Score: 0.0650)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\ntop_k = min(5, len(corpus))\nfor query in queries:\n    query_embedding = embedder.encode(query, convert_to_tensor=True)\n    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n    hits = hits[0]      #Get the hits for the first query\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n    for hit in hits:    \n        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:08:20.266889Z","iopub.execute_input":"2024-04-13T11:08:20.267378Z","iopub.status.idle":"2024-04-13T11:08:20.405590Z","shell.execute_reply.started":"2024-04-13T11:08:20.267345Z","shell.execute_reply":"2024-04-13T11:08:20.404741Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d21ae31b0eea4c0da092d7f53727adba"}},"metadata":{}},{"name":"stdout","text":"\n\n======================\n\n\nQuery: A man is eating pasta.\n\nTop 5 most similar sentences in corpus:\nA man is eating food. (Score: 0.7035)\nA man is eating a piece of bread. (Score: 0.5272)\nA man is riding a horse. (Score: 0.1889)\nA man is riding a white horse on an enclosed ground. (Score: 0.1047)\nA cheetah is running behind its prey. (Score: 0.0980)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"237a146a6cd0407a9767ae1a6a4d11eb"}},"metadata":{}},{"name":"stdout","text":"\n\n======================\n\n\nQuery: Someone in a gorilla costume is playing a set of drums.\n\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.6433)\nA woman is playing violin. (Score: 0.2564)\nA man is riding a horse. (Score: 0.1389)\nA man is riding a white horse on an enclosed ground. (Score: 0.1191)\nA cheetah is running behind its prey. (Score: 0.1080)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17592ad5e35242fcbad3a28c6c721924"}},"metadata":{}},{"name":"stdout","text":"\n\n======================\n\n\nQuery: A cheetah chases prey on across a field.\n\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.8253)\nA man is eating food. (Score: 0.1399)\nA monkey is playing drums. (Score: 0.1292)\nA man is riding a white horse on an enclosed ground. (Score: 0.1097)\nA man is riding a horse. (Score: 0.0650)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}