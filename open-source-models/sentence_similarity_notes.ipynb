{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Dense Vectors Using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029601097106933594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 489,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf579e6103324a7b81bbece2b0ca23ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/489 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04707837104797363,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 539,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713432aa832f418e806d9d067942a200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/539 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.09469461441040039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa664a8ee1d456f9a10174f18d96487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0987100601196289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 466081,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1b6537803f407e9e4a9fca2c03d88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.05955147743225098,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading special_tokens_map.json",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534702a0051d4431b81aeae33a5aa2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0457763671875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 265486777,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3fa034757e4a0eaa4613345731e062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/253M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will be using the sentence-transformers/stsb-distilbert-base model to build our dense vectors\n",
    "# First we initialize our model and tokenizer:\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/stsb-distilbert-base')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/stsb-distilbert-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we tokenize a sentence just as we have been doing before:\n",
    "text = \"hello world what a time to be alive!\"\n",
    "\n",
    "tokens = tokenizer.encode_plus(text, max_length=128,\n",
    "                               truncation=True, padding='max_length',\n",
    "                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n",
       "         [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n",
       "         [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n",
       "         ...,\n",
       "         [-0.7853,  0.8094, -0.2639,  ...,  0.2177,  0.3335,  0.1107],\n",
       "         [-0.7528,  0.6285, -0.0088,  ...,  0.1024,  0.4585,  0.1720],\n",
       "         [-1.0754,  0.4878, -0.3458,  ...,  0.2764,  0.5604,  0.1236]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We process these tokens through our model:\n",
    "outputs = model(**tokens)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dense vector representations of our text are contained within the outputs 'last_hidden_state' tensor, which we access like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n",
       "         [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n",
       "         [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n",
       "         ...,\n",
       "         [-0.7853,  0.8094, -0.2639,  ...,  0.2177,  0.3335,  0.1107],\n",
       "         [-0.7528,  0.6285, -0.0088,  ...,  0.1024,  0.4585,  0.1720],\n",
       "         [-1.0754,  0.4878, -0.3458,  ...,  0.2764,  0.5604,  0.1236]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 128 because max_length = 128\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have produced our dense vectors embeddings, we need to perform a mean pooling operation on them to create a single vector encoding (the sentence embedding). To do this mean pooling operation we will need to multiply each value in our embeddings tensor by it's respective attention_mask value - so that we ignore non-real tokens.\n",
    "\n",
    "To perform this operation, we first resize our attention_mask tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's attention_mask status. Then we multiply the two tensors to apply the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n",
       "         [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n",
       "         [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum the remained of the embeddings along axis 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then sum the number of values that must be given attention in each position of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the mean as the sum of the embedding activations summed divided by the number of values that should be given attention in each position summed_mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8485e-01,  7.8107e-01, -1.7720e-01, -1.4125e+00, -2.3358e-01,\n",
       "          9.0891e-01, -7.8390e-02,  6.0347e-01,  6.7886e-02, -3.9841e-01,\n",
       "          3.9223e-02, -4.6774e-01, -7.1848e-01, -1.1863e-01, -7.1194e-02,\n",
       "          6.6018e-03, -1.4093e-01,  3.1271e-01, -6.5574e-01, -1.6470e-01,\n",
       "         -1.0026e-01, -3.8357e-01,  6.1278e-02, -7.3818e-01, -5.9918e-01,\n",
       "          2.8855e-01,  8.6372e-01,  5.8388e-01, -3.5059e-02,  4.3197e-01,\n",
       "         -5.0111e-01, -4.3498e-01,  2.3498e-01, -3.7127e-01, -1.0044e+00,\n",
       "          1.0000e+00, -2.1000e+00, -3.2251e-01, -1.6085e-01, -7.3701e-01,\n",
       "          5.4928e-01, -1.2066e-01,  7.2698e-01, -5.0328e-02, -1.7545e+00,\n",
       "          8.0573e-01, -5.0553e-01, -4.7172e-01, -1.6727e-01,  5.9727e-01,\n",
       "          5.6203e-01, -3.6104e-01, -1.6429e-01, -5.5215e-01, -5.0418e-01,\n",
       "          5.6187e-01, -1.1415e+00,  1.0771e+00,  5.5689e-01, -7.0632e-02,\n",
       "         -2.6932e-01, -6.8905e-01,  1.8093e-01,  3.1045e-01,  3.9037e-02,\n",
       "          3.1064e-01, -4.4495e-01, -4.7363e-02,  1.7010e+00,  1.2346e-01,\n",
       "          6.7439e-01, -6.4171e-01,  4.6369e-01, -1.0917e+00, -7.6345e-01,\n",
       "         -3.0914e-01, -3.0577e-01,  7.6974e-01, -1.3508e+00, -2.3315e-01,\n",
       "         -3.3893e-01, -7.1898e-01, -1.4306e-01,  5.6030e-01, -6.8665e-01,\n",
       "         -6.2314e-01, -1.6671e-01,  4.8549e-01, -4.1270e-01,  1.0744e+00,\n",
       "          7.6589e-01,  3.2901e-01,  3.7139e-01,  4.4882e-01,  9.8538e-01,\n",
       "          2.9093e-01, -8.5644e-01, -7.6587e-01,  3.4619e-01,  1.7652e-01,\n",
       "          7.8148e-02, -2.3852e-01,  7.3973e-01, -3.3606e-01, -1.3271e-01,\n",
       "         -3.0366e-02,  9.4191e-01, -9.6431e-02, -4.2533e-01, -3.6001e-01,\n",
       "          4.0952e-01, -3.5570e-01, -6.1359e-01,  1.0244e+00, -1.6946e-01,\n",
       "          5.8016e-01, -3.5626e-01, -1.8950e-01, -1.3671e+00, -4.2409e-02,\n",
       "          7.6267e-01, -3.8510e-01, -4.4530e-01,  6.0831e-01, -3.5163e-01,\n",
       "         -3.1655e-01,  5.4221e-01, -4.2779e-02,  1.8298e+00, -5.0056e-02,\n",
       "          4.0329e-01,  3.8894e-01, -1.4479e-01, -5.0510e-01,  4.9653e-01,\n",
       "          2.2225e-01, -1.7006e-01,  1.6015e-01, -5.6461e-01, -6.1269e-01,\n",
       "          6.5336e-01,  7.9370e-01, -1.3359e-01, -2.9642e-01,  6.5500e-01,\n",
       "          4.5574e-01,  1.3021e-01,  8.5983e-01,  9.5492e-02, -4.1140e-01,\n",
       "         -6.0792e-01, -6.8995e-01, -8.0654e-01, -3.3334e-01, -6.1655e-01,\n",
       "         -6.7164e-01, -7.9168e-01, -6.0887e-01, -6.7842e-01,  7.7270e-01,\n",
       "         -1.8380e-01,  4.2944e-01,  8.2487e-01, -3.1597e-01, -1.6402e+00,\n",
       "          8.4033e-01,  6.2706e-01,  4.5629e-01,  3.6232e-01, -4.5201e-01,\n",
       "         -1.8216e-02,  6.0874e-02, -7.5308e-01,  1.0445e+00, -1.2082e+00,\n",
       "         -3.3916e-01,  5.6531e-01, -8.5100e-01, -3.4787e-01,  1.2856e-01,\n",
       "         -5.8088e-01, -5.2811e-01, -1.2420e-02,  6.3881e-02, -6.7477e-02,\n",
       "         -1.7361e-01,  7.0940e-01, -1.4097e+00, -1.3953e-01, -1.9912e-01,\n",
       "          4.7711e-01,  8.0363e-01,  3.7343e-01, -5.2863e-01,  3.9637e-01,\n",
       "          9.4878e-01,  1.2467e-01, -1.7886e-01,  7.9958e-01,  3.6675e-02,\n",
       "          3.2056e-01,  1.0148e+00,  6.4841e-01, -1.6269e-01,  3.3561e-01,\n",
       "         -5.0149e-01,  2.4113e-01, -8.9913e-01, -1.2511e-01, -3.3696e-01,\n",
       "          3.0698e-01, -2.2303e-01, -3.8996e-01, -2.8901e-01, -3.8460e-01,\n",
       "          8.8542e-01, -1.6964e-01,  2.4618e-01, -8.4116e-01, -3.2811e-01,\n",
       "         -3.2727e-01, -2.2230e-02,  4.5131e-01, -4.1267e-01,  9.1945e-01,\n",
       "         -6.3390e-01,  9.4229e-01,  1.3876e-01, -2.5901e-02, -4.5191e-01,\n",
       "          5.7051e-02,  2.4740e-01, -5.7986e-01,  7.7694e-02,  7.6410e-02,\n",
       "         -3.4006e-01,  4.3327e-01, -3.9236e-01,  4.5135e-01, -5.3925e-01,\n",
       "         -5.7638e-01, -5.1190e-01, -2.4838e-02, -2.9940e-01, -2.6119e-01,\n",
       "         -6.8237e-01, -1.0826e-01,  2.7870e-01,  5.2347e-01, -1.2790e+00,\n",
       "         -7.5903e-01, -4.1540e-01,  2.7823e-01,  1.2852e-01,  7.8037e-01,\n",
       "         -8.0996e-01, -7.2413e-01, -8.6791e-01, -8.5757e-01,  1.7594e-01,\n",
       "          4.0083e-01, -7.3396e-01,  3.9002e-01,  1.1243e-01,  1.2089e-01,\n",
       "         -1.0793e-01,  1.1323e-01, -2.7789e-01, -7.9832e-03,  2.1983e-02,\n",
       "         -8.3703e-01,  3.4330e-01,  1.3543e-01,  5.7925e-02,  1.9617e+00,\n",
       "          8.3412e-01, -7.2922e-01,  4.6160e-01, -3.0357e-01, -3.4166e-01,\n",
       "         -6.9856e-01,  9.4906e-02, -1.1436e+00,  4.2824e-02, -3.4007e-01,\n",
       "          1.7994e-01, -6.0481e-01, -9.6435e-01, -4.9273e-01, -1.3865e-01,\n",
       "         -1.5011e+00,  3.8161e-01, -3.7843e-01, -2.7537e-01,  8.0850e-02,\n",
       "          3.0396e-01,  6.3914e-03, -1.0301e+00, -7.4074e-01,  6.2466e-02,\n",
       "          2.3288e-01, -9.3533e-01,  1.0078e-01, -1.2119e+00, -6.6829e-01,\n",
       "         -5.5834e-02, -9.2504e-02, -4.9092e-01,  1.1554e-01,  4.5436e-01,\n",
       "          1.2235e+00,  5.7517e-01, -1.0007e+00,  6.2173e-01, -6.6875e-02,\n",
       "          1.2660e+00,  4.9329e-01, -1.5459e-01,  9.2994e-02, -5.6999e-02,\n",
       "         -2.8559e-02,  5.2734e-01, -3.0664e-01, -8.0383e-01, -1.6655e-01,\n",
       "         -5.5859e-01, -5.1713e-01,  4.6910e-02, -1.1429e+00, -1.6247e-01,\n",
       "         -1.2338e-01, -3.4689e-01, -3.5227e-01, -3.8736e-01,  1.0393e+00,\n",
       "         -1.6471e-01, -1.3882e-01,  9.1784e-01, -7.2758e-01, -1.5185e-01,\n",
       "          2.8702e-01, -2.0967e-01,  5.5545e-01,  1.8944e-01, -5.0340e-01,\n",
       "         -1.0897e+00, -7.5433e-01, -1.3625e+00, -3.8773e-02, -7.7702e-01,\n",
       "          2.5420e-01,  3.1660e-01, -8.6211e-01, -2.3615e-01,  4.6481e-03,\n",
       "          5.1776e-01,  4.7276e-01, -7.3881e-02, -5.5788e-01,  2.4043e-01,\n",
       "          9.5054e-01,  2.6625e-02, -4.6603e-01, -3.3385e-01,  4.4900e-01,\n",
       "          1.1014e+00,  5.9350e-01, -1.2061e-01,  2.1053e-01,  7.3098e-01,\n",
       "         -2.3733e-02,  2.6349e-01,  3.3863e-01,  5.9553e-01, -3.3448e-01,\n",
       "          1.2544e-01,  3.3026e-01, -1.5698e-01, -3.7931e-01, -2.5078e-01,\n",
       "         -2.9495e-01,  8.2592e-02,  1.8376e-01, -6.8231e-01, -8.6328e-02,\n",
       "         -5.7801e-01, -8.4704e-02, -1.4150e-01,  9.1605e-01, -5.6759e-01,\n",
       "         -1.0993e-01, -1.5896e-02,  6.2933e-01,  2.1628e-01,  1.1261e-01,\n",
       "          6.5828e-01,  4.5636e-01,  1.0936e+00,  7.4275e-01, -3.7315e-01,\n",
       "          3.7326e-01,  1.0809e+00, -2.4348e-01, -4.9122e-01,  1.1691e+00,\n",
       "          1.0116e+00, -2.2179e-01, -8.4004e-02,  4.4811e-01,  8.3704e-01,\n",
       "         -1.4922e-01, -7.3480e-02,  2.8369e-01,  5.3243e-01,  3.5504e-02,\n",
       "         -7.2948e-01,  2.2285e-01, -8.1695e-01, -9.8308e-02,  1.6787e-02,\n",
       "         -1.0060e+00,  5.8846e-02, -1.1733e-01, -2.5026e-03,  9.7850e-01,\n",
       "          4.2993e-01,  5.5168e-01,  7.5765e-01,  4.1643e-01, -7.7879e-01,\n",
       "          6.5853e-01, -2.7104e-01, -2.1195e-01,  2.6836e-01,  8.9252e-02,\n",
       "         -2.2026e-01,  7.0055e-01, -5.0542e-01, -9.2811e-01,  2.8497e-01,\n",
       "         -3.2909e-01,  9.0162e-01,  5.6190e-01,  3.8479e-02,  7.6101e-01,\n",
       "         -2.4245e-02, -5.2505e-01,  4.9243e-01, -1.1323e+00, -7.9398e-02,\n",
       "          8.9293e-01, -4.1039e-01, -4.2587e-01,  5.6288e-01, -3.1121e-01,\n",
       "         -5.0377e-02, -7.7956e-01,  7.0310e-01, -1.1243e-01,  3.1637e-01,\n",
       "          1.5981e-01, -9.6210e-03, -1.0382e+00, -1.8747e-03, -9.9495e-02,\n",
       "         -1.5131e+00,  7.5718e-01, -7.7794e-02,  1.0319e+00,  5.2133e-01,\n",
       "         -3.2082e-01, -1.3737e-01,  1.0844e+00, -3.7648e-01, -3.4650e-02,\n",
       "          1.3097e-01, -3.5184e-01, -8.1428e-01,  3.4189e-01,  6.7282e-02,\n",
       "         -1.9175e-01,  2.2250e-01,  4.0790e-01, -4.0172e-02,  9.3394e-01,\n",
       "          1.4848e-01,  9.4150e-02, -6.1521e-01,  1.8073e-01, -9.3871e-01,\n",
       "         -3.5805e-01, -1.1437e-01,  9.8406e-01, -1.3756e+00,  9.7455e-02,\n",
       "         -2.3249e-01,  9.5018e-01, -2.1394e-01, -2.3394e-01,  5.0237e-01,\n",
       "          2.5898e-01, -2.2813e-01, -2.3680e-01, -2.3152e-01, -9.8057e-01,\n",
       "          3.9108e-01,  5.8315e-01,  1.6551e-01, -3.6449e-01,  4.2075e-01,\n",
       "          9.3581e-01, -4.6776e-01, -2.2666e-02,  3.7928e-01, -6.1125e-01,\n",
       "         -4.0730e-01,  9.0755e-01,  1.0523e+00, -1.9673e-01, -6.0428e-02,\n",
       "         -5.5663e-02,  3.8640e-01, -1.2758e-01,  6.1613e-01, -3.9228e-01,\n",
       "          9.0591e-01, -4.3536e-01, -7.4161e-02,  1.0847e-01, -6.4019e-02,\n",
       "          6.2278e-01,  3.7997e-01, -1.1579e-01, -1.9312e+00,  7.1141e-01,\n",
       "          1.1751e-01, -4.1557e-01, -7.7247e-01,  6.3692e-01,  5.3097e-01,\n",
       "          9.7168e-02, -6.8854e-02, -8.8752e-01,  4.2003e-01,  1.4736e-01,\n",
       "          4.4949e-01,  1.0757e-01,  8.5666e-01,  2.1895e-01, -1.4616e-01,\n",
       "         -2.1148e-01,  7.3091e-02,  5.6748e-01,  3.9416e-01,  2.8382e-02,\n",
       "          1.0420e+00, -9.9249e-02, -5.5125e-01,  7.3611e-02,  1.1771e+00,\n",
       "         -5.5362e-01, -1.0581e-01, -4.2232e-01, -1.5856e+00,  7.3779e-01,\n",
       "         -1.4219e-01, -1.0619e+00, -6.8308e-01,  1.3318e-02,  4.1730e-01,\n",
       "         -1.1350e+00,  2.5110e-01,  4.9541e-01,  1.0239e-01, -7.1889e-01,\n",
       "          1.0615e-01,  7.6836e-01, -6.0917e-02, -3.6846e-01,  5.1103e-02,\n",
       "         -6.9368e-01, -1.6377e-01,  7.2992e-01, -2.7181e-01, -1.7474e-01,\n",
       "          6.6675e-01, -2.4677e-01, -2.8554e-01, -7.7832e-02,  8.7495e-02,\n",
       "          2.1369e-01,  8.7279e-01, -9.8811e-02, -5.0639e-01, -4.2866e-01,\n",
       "         -5.1867e-01,  4.2720e-01,  3.1696e-01, -2.9805e-01, -8.3426e-01,\n",
       "         -1.0784e+00, -7.7276e-01,  4.9140e-01,  1.1272e+00,  1.3698e-02,\n",
       "         -6.8531e-02, -1.1509e-01, -6.5638e-01,  7.9699e-01, -2.6068e-01,\n",
       "          1.0395e+00, -4.7972e-01, -1.4439e-01,  7.8087e-01, -5.9054e-01,\n",
       "          2.1602e-01, -7.4450e-01, -1.3328e-01, -1.4613e-01,  8.9815e-01,\n",
       "         -1.0125e+00, -6.5561e-01,  7.6670e-01, -2.8419e-01, -1.3880e-01,\n",
       "         -7.1945e-01, -5.1779e-01,  1.4314e-01, -2.3534e-01, -5.9846e-01,\n",
       "          6.0434e-02, -6.3185e-02, -1.5664e+00, -1.9544e-01,  2.0409e-01,\n",
       "         -1.0337e+00,  9.1216e-01,  2.3952e-01,  1.0881e-01, -2.0045e-01,\n",
       "          8.4616e-01, -7.5020e-02,  3.4787e-01, -1.5094e+00, -2.5039e-01,\n",
       "         -6.5037e-02,  6.9634e-01, -2.6770e-01,  8.9710e-02, -4.8853e-01,\n",
       "          7.0874e-01, -7.6796e-01,  8.4987e-01,  4.1382e-01, -4.0460e-01,\n",
       "          2.8681e-01,  1.0482e+00,  1.6342e-01,  8.9450e-02, -2.9139e-01,\n",
       "         -6.0596e-01, -1.0153e-01, -3.3035e-01, -4.3888e-01, -6.9056e-02,\n",
       "          5.0943e-02,  3.7704e-01,  6.6890e-02,  5.8372e-01,  3.2395e-01,\n",
       "         -2.4983e-01, -2.9541e-01,  3.7929e-01, -3.1190e-01, -1.3260e-01,\n",
       "          5.0000e-01,  6.4270e-01, -3.0923e-01, -2.9641e-01, -8.8432e-01,\n",
       "          4.0610e-01,  6.5061e-01, -1.2178e-02,  1.1644e+00, -5.9447e-01,\n",
       "         -1.8064e-01,  6.1685e-01, -1.1272e-01, -6.0815e-01, -7.2103e-01,\n",
       "          7.6628e-01, -3.2992e-01, -5.0286e-01,  5.1562e-01, -4.1571e-01,\n",
       "         -4.7703e-01, -2.8721e-01,  8.4478e-01, -3.9162e-01,  9.2312e-02,\n",
       "          7.8206e-01,  2.1263e-01, -6.4607e-01,  9.6211e-01,  1.2251e-01,\n",
       "         -9.9896e-01,  1.4946e-01, -2.5206e-01, -3.9582e-01,  8.5691e-01,\n",
       "          3.5398e-01, -8.7718e-01,  2.3607e-01, -2.1513e-01, -6.1426e-01,\n",
       "         -1.4891e-01,  4.2167e-01, -9.3733e-01,  7.1058e-01, -1.5226e-01,\n",
       "          1.3321e+00,  1.2884e-01,  1.4089e-01,  6.7874e-01, -5.8004e-01,\n",
       "          1.5258e-01,  5.5926e-01, -1.4397e-01, -3.4066e-01, -6.2638e-01,\n",
       "          1.5209e-01,  1.1089e-01,  6.4281e-02,  5.5102e-01, -1.1813e-01,\n",
       "         -4.6605e-01,  3.0925e-01, -3.5231e-01,  4.1569e-01,  1.4494e+00,\n",
       "         -3.5271e-01, -9.2626e-01, -3.0457e-01, -2.8597e-01, -4.8825e-01,\n",
       "         -2.0081e-01,  6.2256e-02, -6.3340e-01,  4.3752e-02,  3.5119e-01,\n",
       "          7.5687e-01, -3.1523e-01,  2.9945e-01, -7.5232e-01,  1.2727e-01,\n",
       "         -5.1653e-01, -1.7894e-02,  4.0687e-01, -1.9404e-01,  3.7166e-01,\n",
       "          6.0207e-01,  9.7962e-01,  1.3890e-01,  6.2343e-01,  4.1314e-01,\n",
       "          4.1372e-01,  5.6574e-01, -4.6809e-01]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = summed / summed_mask\n",
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we calculate our dense similarity vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Similarity\n",
    "When calculating similarity between our transformer embedded vectors, we can use any of the three similarity metrics already covered.\n",
    "\n",
    "But first, let's create some embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "    \"It took him a month to finish the meal.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]\n",
    "\n",
    "# thanks to https://randomwordgenerator.com/sentence.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "# initialize dictionary that will contain tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # tokenize sentence and append to dictionary lists\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128, truncation=True,\n",
    "                                       padding='max_length', return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We process these tokens through our model:\n",
    "\n",
    "outputs = model(**tokens)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.9229e-02,  6.2300e-01,  3.5370e-02,  ...,  8.0334e-01,\n",
       "           1.6314e+00,  3.2812e-01],\n",
       "         [ 3.6729e-02,  6.8419e-01,  1.9460e-01,  ...,  8.4759e-02,\n",
       "           1.4747e+00, -3.0080e-01],\n",
       "         [-1.2142e-02,  6.5431e-01, -7.2717e-02,  ..., -3.2600e-02,\n",
       "           1.7717e+00, -6.8121e-01],\n",
       "         ...,\n",
       "         [ 1.9532e-01,  1.1085e+00,  3.3905e-01,  ...,  1.2826e+00,\n",
       "           1.0114e+00, -7.2754e-02],\n",
       "         [ 9.0217e-02,  1.0288e+00,  3.2973e-01,  ...,  1.2940e+00,\n",
       "           9.8650e-01, -1.1125e-01],\n",
       "         [ 1.2404e-01,  9.7365e-01,  3.9329e-01,  ...,  1.1359e+00,\n",
       "           8.7685e-01, -1.0435e-01]],\n",
       "\n",
       "        [[-3.2124e-01,  8.2512e-01,  1.0554e+00,  ..., -1.8555e-01,\n",
       "           1.5169e-01,  3.9366e-01],\n",
       "         [-7.1457e-01,  1.0297e+00,  1.1217e+00,  ...,  3.3118e-02,\n",
       "           2.3820e-01, -1.5632e-01],\n",
       "         [-2.3522e-01,  1.1353e+00,  8.5941e-01,  ..., -4.3096e-01,\n",
       "          -2.7242e-02, -2.9677e-01],\n",
       "         ...,\n",
       "         [-5.4000e-01,  3.2364e-01,  7.8392e-01,  ...,  2.1867e-03,\n",
       "          -2.9941e-01,  2.6594e-01],\n",
       "         [-5.6429e-01,  3.1867e-01,  9.5759e-01,  ...,  3.4248e-02,\n",
       "          -3.0299e-01,  1.8783e-01],\n",
       "         [-5.1719e-01,  3.5987e-01,  9.3357e-01,  ...,  2.4327e-02,\n",
       "          -2.2319e-01,  1.6717e-01]],\n",
       "\n",
       "        [[-7.5756e-01,  8.3988e-01, -3.7922e-01,  ...,  1.2708e-01,\n",
       "           1.2514e+00,  1.3652e-01],\n",
       "         [-6.5908e-01,  7.6135e-01, -4.6619e-01,  ...,  2.2593e-01,\n",
       "           1.1289e+00, -3.6105e-01],\n",
       "         [-9.0070e-01,  6.7913e-01, -3.7775e-01,  ...,  1.1418e-01,\n",
       "           9.0801e-01, -1.8305e-01],\n",
       "         ...,\n",
       "         [-2.1578e-01,  5.4630e-01,  3.1171e-01,  ...,  1.8021e-01,\n",
       "           7.1693e-01, -6.7159e-02],\n",
       "         [-3.0920e-01,  4.8334e-01,  3.0211e-01,  ...,  2.2885e-01,\n",
       "           6.6559e-01, -9.3170e-02],\n",
       "         [-2.9401e-01,  4.6784e-01,  3.0949e-01,  ...,  2.7821e-01,\n",
       "           5.1436e-01, -1.0211e-01]],\n",
       "\n",
       "        [[-1.0246e-01,  9.7842e-01,  1.4798e+00,  ..., -6.7322e-01,\n",
       "          -1.3459e+00, -1.5414e-01],\n",
       "         [ 1.6459e-01,  1.1261e+00,  9.7448e-01,  ..., -8.2403e-01,\n",
       "          -1.5562e+00, -6.0396e-01],\n",
       "         [ 4.7917e-01,  9.7228e-01,  1.3746e+00,  ..., -9.8250e-01,\n",
       "          -1.3523e+00, -5.8834e-01],\n",
       "         ...,\n",
       "         [ 6.3124e-02,  3.3896e-01,  1.2718e+00,  ..., -3.9970e-01,\n",
       "          -1.1031e+00, -1.3408e-01],\n",
       "         [ 1.3678e-01,  4.4807e-01,  1.2677e+00,  ..., -3.7586e-01,\n",
       "          -1.0867e+00, -2.6921e-01],\n",
       "         [ 1.4712e-01,  3.7091e-01,  1.2411e+00,  ..., -3.6103e-01,\n",
       "          -1.1337e+00, -2.6628e-01]],\n",
       "\n",
       "        [[-6.9433e-02,  1.3936e-01,  7.9762e-01,  ...,  1.1904e-01,\n",
       "           9.8823e-01,  2.6582e-01],\n",
       "         [ 5.1373e-03, -5.3535e-02,  8.8652e-01,  ..., -2.0870e-01,\n",
       "           7.9596e-01,  2.9189e-02],\n",
       "         [-1.5181e-01,  1.4075e-02,  7.6035e-01,  ..., -2.6414e-01,\n",
       "           6.3991e-01, -1.5048e-01],\n",
       "         ...,\n",
       "         [-1.6339e-01, -5.6690e-02,  7.4140e-01,  ...,  2.4665e-01,\n",
       "           7.6735e-01,  7.6984e-02],\n",
       "         [-2.2222e-01,  1.7150e-03,  7.0698e-01,  ...,  2.1065e-01,\n",
       "           7.1550e-01,  7.8734e-02],\n",
       "         [-1.9339e-01,  2.5327e-02,  7.8219e-01,  ...,  1.7633e-01,\n",
       "           6.4733e-01,  5.0552e-02]],\n",
       "\n",
       "        [[-2.3620e-01,  8.5513e-01, -8.0395e-01,  ...,  6.1217e-01,\n",
       "           3.0030e-01, -1.4919e-01],\n",
       "         [-8.6805e-02,  9.5311e-01, -6.4188e-01,  ...,  7.8669e-01,\n",
       "           2.9603e-01, -7.3501e-01],\n",
       "         [-3.0156e-01,  1.0148e+00, -3.3798e-01,  ...,  8.6336e-01,\n",
       "           4.6253e-02, -3.6234e-01],\n",
       "         ...,\n",
       "         [-1.0904e-01,  6.3199e-01, -8.4330e-01,  ...,  7.4846e-01,\n",
       "           1.0252e-01,  1.4870e-02],\n",
       "         [ 7.2190e-03,  7.3466e-01, -7.6890e-01,  ...,  6.0643e-01,\n",
       "           1.2874e-01,  3.3143e-02],\n",
       "         [-1.1083e-01,  7.6055e-01, -4.4468e-01,  ...,  6.7188e-01,\n",
       "           1.0593e-01, -3.4437e-03]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dense vector representations of our text are contained within the outputs 'last_hidden_state' tensor, which we access like so:\n",
    "\n",
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have produced our dense vectors embeddings, we need to perform a mean pooling operation on them to create a single vector encoding (the sentence embedding). To do this mean pooling operation we will need to multiply each value in our embeddings tensor by it's respective attention_mask value - so that we ignore non-real tokens.\n",
    "\n",
    "To perform this operation, we first resize our attention_mask tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's attention_mask status. Then we multiply the two tensors to apply the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0692,  0.6230,  0.0354,  ...,  0.8033,  1.6314,  0.3281],\n",
       "         [ 0.0367,  0.6842,  0.1946,  ...,  0.0848,  1.4747, -0.3008],\n",
       "         [-0.0121,  0.6543, -0.0727,  ..., -0.0326,  1.7717, -0.6812],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.3212,  0.8251,  1.0554,  ..., -0.1855,  0.1517,  0.3937],\n",
       "         [-0.7146,  1.0297,  1.1217,  ...,  0.0331,  0.2382, -0.1563],\n",
       "         [-0.2352,  1.1353,  0.8594,  ..., -0.4310, -0.0272, -0.2968],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.7576,  0.8399, -0.3792,  ...,  0.1271,  1.2514,  0.1365],\n",
       "         [-0.6591,  0.7613, -0.4662,  ...,  0.2259,  1.1289, -0.3611],\n",
       "         [-0.9007,  0.6791, -0.3778,  ...,  0.1142,  0.9080, -0.1830],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.1025,  0.9784,  1.4798,  ..., -0.6732, -1.3459, -0.1541],\n",
       "         [ 0.1646,  1.1261,  0.9745,  ..., -0.8240, -1.5562, -0.6040],\n",
       "         [ 0.4792,  0.9723,  1.3746,  ..., -0.9825, -1.3523, -0.5883],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.0694,  0.1394,  0.7976,  ...,  0.1190,  0.9882,  0.2658],\n",
       "         [ 0.0051, -0.0535,  0.8865,  ..., -0.2087,  0.7960,  0.0292],\n",
       "         [-0.1518,  0.0141,  0.7603,  ..., -0.2641,  0.6399, -0.1505],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.2362,  0.8551, -0.8040,  ...,  0.6122,  0.3003, -0.1492],\n",
       "         [-0.0868,  0.9531, -0.6419,  ...,  0.7867,  0.2960, -0.7350],\n",
       "         [-0.3016,  1.0148, -0.3380,  ...,  0.8634,  0.0463, -0.3623],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we sum the remained of the embeddings along axis 1:\n",
    "\n",
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then sum the number of values that must be given attention in each position of the tensor:\n",
    "\n",
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.,  ..., 15., 15., 15.],\n",
       "        [22., 22., 22.,  ..., 22., 22., 22.],\n",
       "        [15., 15., 15.,  ..., 15., 15., 15.],\n",
       "        [16., 16., 16.,  ..., 16., 16., 16.],\n",
       "        [12., 12., 12.,  ..., 12., 12., 12.],\n",
       "        [14., 14., 14.,  ..., 14., 14., 14.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we calculate the mean as the sum of the embedding activations summed divided by the number of values that should be given attention in each position summed_mask:\n",
    "\n",
    "mean_pooled = summed / summed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0745,  0.8637,  0.1795,  ...,  0.7734,  1.7247, -0.1803],\n",
       "        [-0.3715,  0.9729,  1.0840,  ..., -0.2552, -0.2759,  0.0358],\n",
       "        [-0.5030,  0.7950, -0.1240,  ...,  0.1441,  0.9704, -0.1791],\n",
       "        [-0.0132,  0.9773,  1.4516,  ..., -0.8462, -1.4004, -0.4118],\n",
       "        [-0.2019,  0.0597,  0.8603,  ..., -0.0100,  0.8431, -0.0841],\n",
       "        [-0.2131,  1.0175, -0.8833,  ...,  0.7371,  0.1947, -0.3011]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And that is how we calculate our dense similarity vector.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33088917, 0.72192585, 0.17475507, 0.44709665, 0.55483645]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's calculate cosine similarity for sentence 0:\n",
    "\n",
    "# convert from PyTorch tensor to numpy array\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "# calculate\n",
    "cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as intended, the most similar sentence is that in index 2 - which contains the same meaning as our first sentence, without using the same words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've worked through creating our embeddings using the transformers library - and at times it can be quite involved. Now, it's important to understand the steps, but we can make life easier by using the sentence-transformers library.\n",
    "\n",
    "We'll work through the same process - but using sentence-transformers instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentences:\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And no we have our sentence embeddings - a much quicker approach. We then compare just as we did before using cosine similarity:\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33088908, 0.72192574, 0.17475477, 0.44709638, 0.5548362 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's calculate cosine similarity for sentence 0:\n",
    "\n",
    "cosine_similarity(\n",
    "    [sentence_embeddings[0]],\n",
    "    sentence_embeddings[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67d509d7eba20331265163425a81888951fd8dd61b2206587b199305e358c686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
