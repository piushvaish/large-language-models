{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are sentence embeddings?\n",
    "Sentence embeddings can be described as a document processing method of mapping sentences to vectors as a means of representing text with real numbers suitable for machine learning. Similarity measurements such as cosine similarity or Manhattan/Euclidean distance, evaluate semantic textual similarity so that the scores can be exploited for a variety of helpful NLP tasks, including information retrieval, paraphrase identification and text summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers and Pre-trained Language models\n",
    "The revolutionary Transformer model was introduced in 2017 by Vaswani et al. The attention-only architecture was extremely impactful in the field of NLP, and it’s influence is now being felt in other fields such as computer vision, and graph neural networks. For a detailed look at Transformers, Harvard NLP’s Annotated Transformer is a wonderful tutorial. In 2018, building on this architecture, Devlin et al. created BERT (Bidirectional Encoder Representations from Transformers) a pre-trained language model, that set SOTA records for various NLP tasks, including the Semantic Textual Similarity (STS) benchmark (Cer et al. 2017).\n",
    "\n",
    "Following BERT, RoBERTa was released by Lui et al. 2019, and this model uses a robustly optimized pre-training approach to improve upon the original BERT model. These pre-trained language models are powerful tools, and this paradigm has extended to other languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/transformer-based-sentence-embeddings-cd0935b3b1e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Strategy\n",
    "S-BERT importantly adds a pooling operation to the output of a BERT/RoBERTA model to create a fixed-sized sentence embedding. As mentioned, the default is a MEAN pooling strategy, since this was determined to be superior to using the output of the [CLS]-token or a MAX pooling strategy. A fixed-sized sentence embedding is the key to producing embeddings that can be used efficiently in downstream tasks, such as inferring semantic textual similarity with cosine similarity scores.\n",
    "\n",
    "Inference with Sentence-BERT\n",
    "Once trained, S-BERT uses a regressive objective function for inference within a siamese network similar to the one used for fine-tuning. As seen in the diagram below, the cosine similarity between two sentence embeddings (u and v) are computed as a score between [-1…1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regressive objective function is optimized with mean-squared-error loss, and concatenation is not required before calculating the cosine similarity of the sentence embeddings.\n",
    "\n",
    "##### Implementation\n",
    "There are two main options available to produce S-BERT or S-RoBERTa sentence embeddings, the Python library Huggingface transformers or a Python library maintained by UKP Lab, sentence-transformers. Both libraries rely on Pytorch, and the main difference is that using Huggingface requires defining a function for the pooling strategy, as seen in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://www.sbert.net/examples/applications/computing-embeddings/README.html\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create S-BERT sentence embeddings with Huggingface, simply import the Autotokenizer and Automodel to tokenize and create a model from the pre-trained S-BERT model (a BERT-base model that was fine-tuned on a natural language inference dataset). As seen in the code snippet below, Pytorch is used to compute the embeddings, and the previously defined MEAN pooling function is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Once trained, S-BERT uses a regressive objective function for inference within a siamese network similar to the one used for fine-tuning.',' As seen in the diagram below, the cosine similarity between two sentence embeddings (u and v) are computed as a score between [-1…1].']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive text summarization with sentence-transformers\n",
    "If using sentence-transformers, there are several pre-trained S-BERT and S-RoBERTa models available for sentence embeddings. For the task of extractive text summarization, I prefer to use a distilled version of S-RoBERTa fine-tuned on a paraphrase identification dataset. As seen in the code snippet below, with sentence-transformers it is simple to create a model and embeddings, and then calculate the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\piush\\Desktop\\nlp-notes\\nlp\\notebooks\\transformer_based_sentence_embeddings.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/piush/Desktop/nlp-notes/nlp/notebooks/transformer_based_sentence_embeddings.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(sentences, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/piush/Desktop/nlp-notes/nlp/notebooks/transformer_based_sentence_embeddings.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# calculate pair-wise cosine similarities\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/piush/Desktop/nlp-notes/nlp/notebooks/transformer_based_sentence_embeddings.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m cosine_scores \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mpytorch_cos_sim(embeddings, embeddings)\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'util' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilroberta-base-paraphrase-v1')\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "# calculate pair-wise cosine similarities\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating cosine similarity, I use code adapted from the Python library LexRank to find the most central sentences in the document, as calculated by degree centrality. Lastly, to produce a summary I select the top five highest ranked sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LexRank import degree_centrality_scores\n",
    "\n",
    "centrality_scores = degree_centrality_scores(cosine_scores, threshold=None)\n",
    "\n",
    "# argsort to order by sentence score\n",
    "most_central_sentence_indices = np.argsort(centrality_scores)\n",
    "\n",
    "# Print the 5 sentences with the highest scores\n",
    "print(\"\\n\\nSummary:\")\n",
    "for idx in most_central_sentence_indices[0:4]:\n",
    "    print(all_sentences_pooled[idx].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67d509d7eba20331265163425a81888951fd8dd61b2206587b199305e358c686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
