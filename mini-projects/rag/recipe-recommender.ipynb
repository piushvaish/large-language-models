{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Building A RAG System with Free GPU, MongoDB and Open Source Models\n### Step 1: Installing Libraries\nThe shell command sequence below installs libraries for leveraging open-source large language models (LLMs), embedding models, and database interaction functionalities. These libraries simplify the development of a RAG system, reducing the complexity to a small amount of code:\n\n* PyMongo: A Python library for interacting with MongoDB that enables functionalities to connect to a cluster and query data stored in collections and documents.\n* Pandas: Provides a data structure for efficient data processing and analysis using Python\n* Hugging Face datasets: Holds audio, vision, and text datasets\n* Hugging Face Accelerate: Abstracts the complexity of writing code that leverages hardware accelerators such as GPUs. * Accelerate is leveraged in the implementation to utilise the Gemma model on GPU resources.\n* Hugging Face Transformers: Access to a vast collection of pre-trained models\n* Hugging Face Sentence Transformers: Provides access to sentence, text, and image embeddings.","metadata":{}},{"cell_type":"code","source":"!pip install -qq datasets transformers accelerate bitsandbytes sentence_transformers \"pymongo[srv]\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Data sourcing and preparation\n\n[Data](https://huggingface.co/datasets/Hieu-Pham/kaggle_food_recipes)","metadata":{}},{"cell_type":"code","source":"# Load Dataset\nfrom datasets import load_dataset\nimport pandas as pd\n\n# https://huggingface.co/datasets/AIatMongoDB/embedded_movies\ndataset = load_dataset(\"Hieu-Pham/kaggle_food_recipes\")\n\n# Convert the dataset to a pandas dataframe\ndataset_df = pd.DataFrame(dataset[\"train\"])\n\ndataset_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset_df.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Preparation\n#https://huggingface.co/docs/datasets/v2.19.0/how_to\n# Remove data point where instructions is missing\ndataset_df = dataset_df.dropna(subset=[\"Instructions\"])\nprint(\"\\nNumber of missing values in each column after removal:\")\nprint(dataset_df.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df[\"Instructions\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df = dataset_df[:100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3: Generating embeddings\nThe steps in the code snippets are as follows:\n1. Import the SentenceTransformer class to access the embedding models.\n2. Load the embedding model using the SentenceTransformer constructor to instantiate the gte-large embedding model.\n3. Define the get_embedding function, which takes a text string as input and returns a list of floats representing the embedding. The function first checks if the input text is not empty (after stripping whitespace). If the text is empty, it returns an empty list. Otherwise, it generates an embedding using the loaded model.\n4. Generate embeddings by applying the get_embedding function to the “Instruction” column of the dataset_df DataFrame, generating embeddings for each recipe’s instruction. The resulting list of embeddings is assigned to a new column named embedding.\n\n<i>Note: It’s not necessary to chunk the text in the full plot, as we can ensure that the text length remains within a manageable range.</i>\n","metadata":{}},{"cell_type":"code","source":"#embed instructions\nfrom sentence_transformers import SentenceTransformer\n\n# https://huggingface.co/thenlper/gte-large\nembedding_model = SentenceTransformer(\"thenlper/gte-large\")\n\n\ndef get_embedding(text: str) -> list[float]:\n    if not text.strip():\n        print(\"Attempted to get embedding for empty text.\")\n        return []\n\n    embedding = embedding_model.encode(text)\n\n    return embedding.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df[\"embedding\"] = dataset_df[\"Instructions\"].apply(get_embedding)\ndataset_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop columns not required\ndataset_df = dataset_df.drop(columns=[\"Unnamed: 0\",\"Image_Name\", \"Ingredients\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 4: Database setup and connection\nMongoDB acts as both an operational and a vector database. It offers a database solution that efficiently stores, queries and retrieves vector embeddings—the advantages of this lie in the simplicity of database maintenance, management and cost.\n\nTo create a new MongoDB database, set up a database cluster:\n\n1. Head over to MongoDB official site and register for a [free MongoDB Atlas account](https://www.mongodb.com/cloud/atlas/register?utm_campaign=devrel&utm_source=community&utm_medium=cta&utm_content=Partner%20Cookbook&utm_term=richmond.alake), or for existing users, sign into [MongoDB Atlas](https://account.mongodb.com/account/login?utm_campaign=devrel&utm_source=community&utm_medium=cta&utm_content=Partner%20Cookbook&utm_term=richmond.alakee).\n\n2. Select the ‘Database’ option on the left-hand pane, which will navigate to the Database Deployment page, where there is a deployment specification of any existing cluster. Create a new database cluster by clicking on the “+Create” button.\n\n3. Select all the applicable configurations for the database cluster. Once all the configuration options are selected, click the “Create Cluster” button to deploy the newly created cluster. MongoDB also enables the creation of free clusters on the “Shared Tab”.\n\n<i>Note: Don’t forget to whitelist the IP for the Python host or 0.0.0.0/0 for any IP when creating proof of concepts.</i>\n\n4. After successfully creating and deploying the cluster, the cluster becomes accessible on the ‘Database Deployment’ page.\n\n5. Click on the “Connect” button of the cluster to view the option to set up a connection to the cluster via various language drivers.\n\nRequires the cluster’s URI(unique resource identifier). \n\n### 4.1 Database and Collection Setup\nBefore moving forward, ensure the following prerequisites are met\n\n* Database cluster set up on MongoDB Atlas\n* Obtained the URI to your cluster\nFor assistance with database cluster setup and obtaining the URI, [refer to our guide for setting up a MongoDB cluster](https://www.mongodb.com/docs/guides/atlas/cluster/) and getting your [connection string](https://www.mongodb.com/docs/guides/atlas/connection-string/)\n\nOnce you have created a cluster, create the database and collection within the MongoDB Atlas cluster by clicking + Create Database in the cluster overview page.\n\nHere is a guide for creating a [database and collection](https://www.mongodb.com/basics/create-database)\n\nThe database will be named recipes.\n\nThe collection will be named instructions.\n\n### Step 5: Create a Vector Search Index\nAt this point make sure that your vector index is created via MongoDB Atlas.\n\nThis next step is mandatory for conducting efficient and accurate vector-based searches based on the vector embeddings stored within the documents in the instructions collection.\n\nCreating a Vector Search Index enables the ability to traverse the documents efficiently to retrieve documents with embeddings that match the query embedding based on vector similarity.\n\nGo here to read more about [MongoDB Vector Search Index](https://www.mongodb.com/docs/atlas/atlas-search/field-types/knn-vector/).\n\n```\n{\n \"fields\": [{\n     \"numDimensions\": 1024,\n     \"path\": \"embedding\",\n     \"similarity\": \"cosine\",\n     \"type\": \"vector\"\n   }]\n}\n```\n\nThe 1024 value of the numDimension field corresponds to the dimension of the vector generated by the gte-large embedding model. If you use the gte-base or gte-small embedding models, the numDimension value in the vector search index must be set to 768 and 384, respectively.\n\n### Step 6: Establish Data Connection\nThe code snippet below also utilises PyMongo to create a MongoDB client object, representing the connection to the cluster and enabling access to its databases and collections.","metadata":{}},{"cell_type":"code","source":"import pymongo\nfrom pymongo.mongo_client import MongoClient\nfrom pymongo.server_api import ServerApi\n\ndef get_mongo_client(mongo_uri):\n    \"\"\"Establish connection to the MongoDB.\"\"\"\n    try:\n        client = MongoClient(mongo_uri, server_api=ServerApi('1'))\n        print(\"Connection to MongoDB successful\")\n        return client\n    except pymongo.errors.ConnectionFailure as e:\n        print(f\"Connection failed: {e}\")\n        return None\n\nURI = uri\nmongo_uri = uri\n\nif not mongo_uri:\n    print(\"MONGO_URI not set in environment variables\")\n\nmongo_client = get_mongo_client(mongo_uri)\n\n# Ingest data into MongoDB\ndb = mongo_client[\"recipes\"]\ncollection = db[\"instructions\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete any existing records in the collection\n#collection.delete_many({})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ingesting data into a MongoDB collection from a pandas DataFrame is a straightforward process that can be efficiently accomplished by converting the DataFrame into dictionaries and then utilising the insert_many method on the collection to pass the converted dataset records.","metadata":{}},{"cell_type":"code","source":"# https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-quick-start/\ndocuments = dataset_df.to_dict(\"records\")\ncollection.insert_many(documents)\nprint(\"Data ingestion into MongoDB completed\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is vector search?\nVector search is a capability that allows you to do semantic search where you are searching data based on meaning. This technique employs machine learning models, often called encoders, to transform text, audio, images, or other types of data into high-dimensional vectors. These vectors capture the semantic meaning of the data, which can then be searched through to find similar content based on vectors being “near” one another in a high-dimensional space. \n\nThis can be a great compliment to traditional keyword-based search techniques but is also seeing an explosion of excitement because of its relevance to augment the capabilities of large language models (LLMs) by providing ground truth outside of what the LLMs “know.” In search use cases, this allows you to find relevant results even when the exact wording isn't known. This technique can be useful in a variety of contexts, such as natural language processing and recommendation systems.\n\n#### Benefits of vector search with MongoDB\nEfficiency: By storing the vectors together with the original data, you avoid the need to sync data between your application database and your vector store at both query and write time.\n1. Consistency: Storing the vectors with the data ensures that the vectors are always associated with the correct data. This can be important in situations where the vector generation process might change over time. By storing the vectors, you can be sure that you always have the correct vector for a given piece of data.\n2. Simplicity: Storing vectors with the data simplifies the overall architecture of your application. You don't need to maintain a separate service or database for the vectors, reducing the complexity and potential points of failure in your system.\n3. Scalability: With the power of MongoDB Atlas, vector search on MongoDB scales horizontally and vertically, allowing you to power the most demanding workloads.","metadata":{}},{"cell_type":"markdown","source":"### Step 7: Perform Vector Search on User Queries\nThe following step implements a function that returns a vector search result by generating a query embedding and defining a MongoDB aggregation pipeline.\n\nThe pipeline, consisting of the \n* vectorSearch and \n* project stages, \n\nexecutes queries using the generated vector and formats the results to include only the required information, such as plot, title, and genres while incorporating a search score for [each result](https://huggingface.co/learn/cookbook/en/rag_with_hugging_face_gemma_mongodb).","metadata":{}},{"cell_type":"code","source":"def vector_search(user_query, collection):\n    \"\"\"\n    Perform a vector search in the MongoDB collection based on the user query.\n\n    Args:\n    user_query (str): The user's query string.\n    collection (MongoCollection): The MongoDB collection to search.\n\n    Returns:\n    list: A list of matching documents.\n    \"\"\"\n\n    # Generate embedding for the user query\n    query_embedding = get_embedding(user_query)\n\n    if query_embedding is None:\n        return \"Invalid query or embedding generation failed.\"\n\n    # Define the vector search pipeline\n    pipeline = [\n        {\n            \"$vectorSearch\": {\n                \"index\": \"vector_index\",\n                \"queryVector\": query_embedding,\n                \"path\": \"embedding\",\n                \"numCandidates\": 100,  # Number of candidate matches to consider (100 sample)\n                \"limit\": 6,  # Return top 6 matches\n            }\n        },\n        {\n            \"$project\": {\n                \"_id\": 0,  # Exclude the _id field\n                \"Title\": 1,  # Include the title field\n                \"Cleaned_Ingredients\": 1,\n                \"Instructions\": 1,  # Include the Instructions field\n                \"score\": {\"$meta\": \"vectorSearchScore\"},  # Include the search score\n            }\n        },\n    ]\n\n\n    # Execute the search\n    results = collection.aggregate(pipeline)\n    return list(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 8: Handling user queries","metadata":{}},{"cell_type":"code","source":"def get_search_result(query, collection):\n\n    get_knowledge = vector_search(query, collection)\n\n    search_result = \"\"\n    for result in get_knowledge:\n        search_result += f\"\"\"Title: {result.get('Title', 'N/A')}, Ingredients: {result.get('Cleaned_Ingredients', 'N/A')}, \n                           \"Instructions\": {result.get('Instructions', 'N/A')}\\n\"\"\"\n    return search_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conduct query with retrival of sources\nquery = \"Which is the best recipe for a light and healthy meal for lunch?\"\nsource_information = get_search_result(query, collection)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login, Repository\n\n# Login to Hugging Face\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Understanding Mistral 7B\nMistral 7B is a new 7.3 billion parameter language model that represents a major advance in large language model (LLM) capabilities. It has outperformed the 13 billion parameter Llama 2 model on all tasks and outperforms the 34 billion parameter Llama 1 on many benchmarks.\n\nWe will create 4-bit quantization with NF4-type configuration using BitsAndBytes to load our model in 4-bit precision. It will help us load the model faster and reduce the memory footprint so that it can be run on [Google Colab](https://www.datacamp.com/tutorial/mistral-7b-tutorial) or consumer GPUs.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import BitsAndBytesConfig\nimport torch\nimport time\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    llm_int8_enable_fp32_cpu_offload=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n                                             device_map=\"auto\", \n                                             quantization_config=bnb_config)\n                                             \ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          use_fast=True, \n                                          quantization_config=bnb_config) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \n     \"content\": f\"\"\"\n     Answer the query by using the source information.\n     #Query\n    {query}\n    /n\n    #Source Information\n    {source_information}\n    Give only one recipe.\n     \"\"\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\ndevice = \"cuda\" \nmodel_inputs = encodeds.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0].split('[/INST]')[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the source information provided, here's the Kimchi Toast recipe:\n\nIngredients:\n- 4 oz. cream cheese, room temperature\n- ¾ cup finely chopped kimchi; plus more for serving (optional)\n- 2 scallions, thinly sliced\n- 1 cup cilantro leaves with tender stems\n- ½ lime\n- Kosher salt\n- 4 (¾\"-thick) slices country-style bread, grilled or toasted\n- Chili oil and toasted white sesame seeds (for serving)\n\nInstructions:\n1. Mix cream cheese and kimchi in a medium bowl.\n2. Toss scallions and cilantro in a small bowl to combine.\n3. Squeeze in juice from lime, season with salt, and toss again.\n4. Smear kimchi cream cheese over each slice of bread.\n5. Top with scallion salad.\n6. Drizzle with chili oil.\n7. Sprinkle with sesame seeds.\n\nEnjoy your light and healthy lunch of Kimchi Toast!</s>","metadata":{}}]}