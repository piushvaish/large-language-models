{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking Strategies for Large Language Models\n",
    "\n",
    "It’s important to note that when dealing with large input text documents, such as PDFs or .txt files, querying the indexes may yield subpar results. To address these performance issues, several factors can be controlled, one of which is the chunking or node creation process within Llama-Index. \n",
    "\n",
    "* https://medium.com/@bavalpreetsinghh/llama-index-a-comprehensive-guide-for-building-and-querying-document-indexes-27a13bb482a5\n",
    "* https://medium.com/@bavalpreetsinghh/llamaindex-chunking-strategies-for-large-language-models-part-1-ded1218cfd30\n",
    "\n",
    "LLamaIndex addresses the challenges of scaling language models to large document collections. To overcome the challenge, LLamaIndex employs two key strategies. Firstly, it chunks documents into smaller contexts such as sentences or paragraphs, which are referred to as Nodes. These Nodes can be efficiently processed by language models. Secondly, LLamaIndex indexes these Nodes using vector embeddings, enabling fast and semantic search.\n",
    "\n",
    "By chunking documents and leveraging vector embeddings, LLamaIndex enables scalable semantic search over large datasets. It achieves this by retrieving relevant Nodes from the index and synthesizing responses using a language model.\n",
    "\n",
    "We cannot pass unlimited data to the application due to two main reasons:\n",
    "\n",
    "1. Context limit: Language models have limited context windows.\n",
    "2. Signal to noise ratio: Language models are more effective when the information provided is relevant to the task.\n",
    "\n",
    "\n",
    "Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser\n",
    "* https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125\n",
    "\n",
    "chunking is the process of breaking down large pieces of text into smaller segments. It’s an essential technique that helps optimize the relevance of the content we get back from a database once we use the LLM to embed content. Some of the strategies involved are\n",
    "\n",
    "1. Fixed-size chunking. This is the most common and straightforward approach to chunking: we simply decide the number of tokens in our chunk and, optionally, whether there should be any overlap between them. Easy to implement & most commonly used, but never makes it to a production setting because the output is satisfactory in a Proof of Concept (POC) setup, but its accuracy degrades as we conduct further testing.\n",
    "\n",
    "2. “Content-aware” chunking. Set of methods for taking advantage of the nature of the content we’re chunking and applying more sophisticated chunking to it. Challenging to implement due to the reasons mentioned above, but if tackled correctly, it could be the most ideal building block for a production-grade Information Retrieval (IR) engine.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-file\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import PDFReader\n",
    "\n",
    "# PDF Reader with `SimpleDirectoryReader`\n",
    "parser = PDFReader()\n",
    "file_extractor = {\"AICompanionsReduceLoneliness.pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"data\", \n",
    "    file_extractor=file_extractor\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_text(data):\n",
    "    \"\"\"Extract the 'text' value from the given data structure.\"\"\"\n",
    "    return next(item[1] for item in data if item[0] == 'text')\n",
    "\n",
    "def remove_citations(text):\n",
    "    \"\"\"Remove citations from the given text.\"\"\"\n",
    "    # Remove citations like (Author Year)\n",
    "    text = re.sub(r'\\([^()]*\\d{4}[^()]*\\)', '', text)\n",
    "    \n",
    "    # Remove citations like (e.g., Author and Author Year; Author et al. Year)\n",
    "    text = re.sub(r'\\(e\\.g\\.,\\s[^()]*\\d{4}[^()]*\\)', '', text)\n",
    "    \n",
    "    # Remove any remaining citations like (Author et al.)\n",
    "    text = re.sub(r'\\([^()]*et al\\.[^()]*\\)', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_text_all = []\n",
    "for i in range(2,75):\n",
    "    # Extract text\n",
    "    extracted_text = extract_text(documents[i])\n",
    "\n",
    "    # Remove citations\n",
    "    cleaned_text = remove_citations(extracted_text)\n",
    "\n",
    "    cleaned_text_all.append(cleaned_text)\n",
    "\n",
    "# print(\"Original text:\")\n",
    "# print(extracted_text)\n",
    "\n",
    "# print(\"\\nText with citations removed:\")\n",
    "# print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents/\n",
    "from llama_index.core import Document\n",
    "cleaned_documents = [Document(text=t) for t in cleaned_text_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: bb5abe72-6f60-418a-b4ff-030be6bafe2e\n",
      "Text: human-human interaction or human-robot interaction scenarios\n",
      "within the context of the con ﬁdant relationship, animal-assisted\n",
      "therapy, increasing social forms of video gaming), (3) increasing\n",
      "opportunities for social contact (face-to-face or online\n",
      "meetings,social prescribing service, asset-based community\n",
      "development),and (4) changing maladapt...\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_documents[72])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Splitters\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/\n",
    "\n",
    "SemanticSplitterNodeParser\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/\n",
    "* https://www.youtube.com/watch?v=8OJC21T2SL4&t=1933s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer size: For a given sentence, the buffer size defines the number of surrounding sentences to be added for embeddings creation. For example, a buffer size of 1 results in 3 sentences (current, previous and next sentence) to be combined and embedded. This parameter can influence how much text is examined together to determine the boundaries of each chunk, impacting the granularity and coherence of the resulting chunks. A larger buffer size might capture more context but can also introduce noise, while a smaller buffer size might miss important context but ensures more precise chunking.\n",
    "\n",
    "Breakpoint percentile threshold: The percentile threshold of sentence distance/dissimilarity to draw breakpoints between sentences. A higher threshold requires sentences to be more distinguishable in order to be split into different chunks. A higher threshold results in fewer chunks and typically larger average chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding()\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")\n",
    "\n",
    "# also baseline splitter\n",
    "base_splitter = SentenceSplitter(chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(cleaned_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may also apply the same social norms of human -human interactions to their interactions with computers . In the domain of consumer -brand relationships , consumers can build relationships with brands via similar process es that they use to build relationships with other people , and these brand relationships can affect their subjective experiences and behavior s . We complement these research streams by conside ring consumer behavioral interactions with AI companions , which are literately, rather than just figuratively, designed and optimized for social relationships . Here we investigate whether interacting with such AI alleviate s loneliness . Can AI companions Help Cope with Loneliness? Loneliness is a state of subjective , aversive solitude characterized by a discrepancy between actual and desired levels of social connection . Loneliness is often not problematic, with almost everyone experiencing loneliness from time to time . Yet some people are not successful at alleviating loneliness, leading to a state of chronic loneliness that is associated with depression , anxiety, and physical health outcomes at levels worse than obesity . The size of the population suffering from chronic loneliness is both sizable and increasing, with estimates in the U.S. ranging from 30% to 60% . The U.S., U.K. and Japan have all identified loneliness as a health pandemic, assigning Ministers of Loneliness (in the U.K. and Japan) and the U .S. \n"
     ]
    }
   ],
   "source": [
    "print(nodes[8].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare against Baseline\n",
    "\n",
    "In contrast let's compare against the baseline with a fixed chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes = base_splitter.get_nodes_from_documents(cleaned_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underscore s the value of empathetic AI interactions , showing that a rtificial empathy narrows the customer experience gap between AI and human agents, with high empathy levels resulting in comparable affective and social experiences to humans , particularly improving social interactions . Another study found that an initial warm (vs. competent) message from chatbots significantly enhances consumers ’ brand perception, creating a closer brand connection and increasing the likelihood of engag ing with the chatbot . Academic studies aside, t he very fact that AI companions with empathic personalities have garnered so many users suggests that consumers are gaining social benefits from these apps, which are also marketed as being caring. For example, Replika advertises that it is “ here to make you feel HEARD, because it genuinely cares about you ” . Apart from feeling heard, another factor that could affect loneliness alleviation is the chatbot’s performance, which consists of a range of features pertaining to managing the conversation effectively, including: timely responses, perceived credibility, context tracking, response variability, and domain knowledge . However, we hypothesize that feeling heard is more critical in alleviating loneliness after AI companion usage compared to communication performance , because one of the primary source s of loneliness is the perceived lack of social and emotional support . In sum, previous research on the impact of interpersonal relationships on loneliness alleviation emphasiz es the critical role of fe eling heard and understood ; however, these studies focus exclusively on human -human relationships and do not address experiences with AI companions. To address this gap , our work causally investigates the effectiveness of AI\n"
     ]
    }
   ],
   "source": [
    "print(base_nodes[8].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vector_index = VectorStoreIndex(base_nodes)\n",
    "base_query_engine = base_vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Tell me 10 different new and unique findings that will help to start a new business in combating social isolation using AI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The need for evidence on new technological solutions in combating loneliness.\n",
      "2. Most existing work in this space is correlational and qualitative.\n",
      "3. The effectiveness of AI-based companions in reducing loneliness.\n",
      "4. Experimental studies using state-of-the-art LLMs to isolate the impact of AI companions.\n",
      "5. AI companions are shown to be more effective than other common technological solutions.\n",
      "6. The effectiveness of AI companions at both cross-sectional and longitudinal scales.\n",
      "7. Chatbots engaging in sophisticated conversations in the domain of relationships.\n",
      "8. The potential of chatbots as a coping solution for societal loneliness.\n",
      "9. Limited insight from behavioral research on the effectiveness of AI applications in alleviating loneliness.\n",
      "10. The importance of innovative AI solutions in combating social isolation.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The findings of study 3 indicate that participants who engaged in the chat interface experienced a higher decrease in loneliness. Additionally, there was no significant main effect of the number of words on the difference in loneliness.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What are the findings of study 3\")\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
