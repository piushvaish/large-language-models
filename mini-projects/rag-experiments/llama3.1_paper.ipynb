{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "1. Decide on your LLM - GPT-4o-mini\n",
    "2. select your embedding\n",
    "3. Document reader\n",
    "4. Index\n",
    "5. Storing\n",
    "6. Query Engine or Chat Engine\n",
    "7. Evaluation\n",
    "8. Observability\n",
    "\n",
    "#### A Note on Tokenization#\n",
    "By default, LlamaIndex uses a global tokenizer for all token counting. This defaults to cl100k from tiktoken, which is the tokenizer to match the default LLM gpt-3.5-turbo.\n",
    "\n",
    "If you change the LLM, you may need to update this tokenizer to ensure accurate token counts, chunking, and prompting.\n",
    "\n",
    "The tokenizer used by LLaMA is a SentencePiece Byte-Pair Encoding tokenizer. Note that this is a tokenizer for LLaMA models, and it's different than the tokenizers used by OpenAI models.\n",
    "\n",
    "#### Resources\n",
    "* https://docs.llamaindex.ai/en/stable/module_guides/models/llms/\n",
    "\n",
    "You can set a global tokenizer like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update in October 2023, Llama 3.1 refers to a version of the LLaMA (Large Language Model Meta AI) series developed by Meta (formerly Facebook). The LLaMA models are designed for various natural language processing tasks and are known for their efficiency and performance. Each version typically includes improvements in architecture, training data, and capabilities compared to its predecessors.\n",
      "\n",
      "If you have specific questions about Llama 3.1 or its features, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# non-streaming\n",
    "resp = OpenAI(model=\"gpt-4o-mini\").complete(\"Llama 3.1 is \")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# tiktoken\n",
    "import tiktoken\n",
    "\n",
    "Settings.tokenizer = tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    "\n",
    "# huggingface\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# Settings.tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search!\n",
    "\n",
    "At a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs.\n",
    "\n",
    "By default, LlamaIndex uses text-embedding-ada-002 from OpenAI.\n",
    "\n",
    "When calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings.\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# global\n",
    "Settings.embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleDirectoryReader is the most commonly used data connector that just works.\n",
    "By default, it can be used to parse a variety of file-types on your local filesystem into a list of Document objects. Additionaly, it can also be configured to read from a remote filesystem just as easily! This is made possible through the fsspec protocol.\n",
    "\n",
    "[Resource](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/data_connectors/simple_directory_reader_remote_fs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder in content called data and upload files\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir='data',\n",
    "    recursive=True,  # recursively searches all subdirectories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 92 docs\n"
     ]
    }
   ],
   "source": [
    "docs = reader.load_data()\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "#docs[:71] #you exclude all the refrences and contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the metadata of each document\n",
    "# for idx, doc in enumerate(docs):\n",
    "#     print(f\"{idx} - {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "This suggests that a chunk size of 1024 might strike an optimal balance between response time and the quality of the responses, measured in terms of faithfulness and relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "# https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# global\n",
    "Settings.chunk_size = 1024\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying\n",
    "Querying is the most important part of your LLM application.\n",
    "\n",
    "Right now, we support the following options:\n",
    "\n",
    "refine: create and refine an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per Node/retrieved chunk.\n",
    "Details: the first chunk is used in a query using the text_qa_template prompt. Then the answer and the next chunk (as well as the original question) are used in another query with the refine_template prompt. And so on until all chunks have been parsed.\n",
    "\n",
    "#### Resources:\n",
    "* https://docs.llamaindex.ai/en/stable/module_guides/querying/\n",
    "* https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Llama 3.1 is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3.1 is a set of foundation models for language that includes models with 8B, 70B, and 405B parameters. These models support multilinguality, coding, reasoning, and tool usage. The development process optimized for data, scale, and managing complexity to enhance the quality and performance of the models.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stream response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a set of foundation models for language that natively support multilinguality, coding, reasoning, and tool usage."
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(streaming=True)\n",
    "streaming_response = query_engine.query(\"Llama 3.1 is\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-Level Composition API\n",
    "* https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/usage_pattern/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a set of foundation models for language that natively support multilinguality, coding, reasoning, and tool usage.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    #verbose=True,\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"Llama 3.1 is\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "LlamaIndex offers key modules to measure the quality of generated results. We also offer key modules to measure retrieval quality.\n",
    "\n",
    "Response Evaluation: Does the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelines?\n",
    "Retrieval Evaluation: Are the retrieved sources relevant to the query?\n",
    "\n",
    "Response Evaluation#\n",
    "Evaluation of generated results can be difficult, since unlike traditional machine learning the predicted result isn't a single number, and it can be hard to define quantitative metrics for this problem.\n",
    "\n",
    "LlamaIndex offers LLM-based evaluation modules to measure the quality of results. This uses a \"gold\" LLM (e.g. GPT-4) to decide whether the predicted answer is correct in a variety of ways.\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/module_guides/evaluating/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you are in a Jupyter Notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# The FaithfulnessEvaluator evaluates if the answer is faithful to the retrieved contexts (in other words, whether if there's hallucination).\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# create llm\n",
    "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "\n",
    "# define evaluator\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "# query index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"LLama 3.1 is\"\n",
    ")\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "print(str(eval_result.passing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/learn/cookbook/en/llm_judge\n",
    "* https://huggingface.co/learn/cookbook/en/rag_evaluation - Excellent Diagram to study Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "response_str = response.response\n",
    "for source_node in response.source_nodes:\n",
    "    eval_result = evaluator.evaluate(\n",
    "        response=response_str, contexts=[source_node.get_content()]\n",
    "    )\n",
    "    print(str(eval_result.passing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
