{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-file\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import PDFReader\n",
    "\n",
    "# PDF Reader with `SimpleDirectoryReader`\n",
    "parser = PDFReader()\n",
    "file_extractor = {\"AICompanionsReduceLoneliness.pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"data\", \n",
    "    file_extractor=file_extractor\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to Choose the Right Embedding Model for Your LLM Application](https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/)\n",
    "\n",
    "There are many fields in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) that we can mostly ignore. The fields that matter most for those of us using these models in the real world are:\n",
    "\n",
    "* Score: the score we should focus on is \"average\" and \"retrieval average\". Both are highly correlated, so focusing on either works.\n",
    "* Sequence length tells us how many tokens a model can consume and compress into a single embedding. Generally speaking, we wouldn't recommend stuffing more than a paragraph of heft into a single embedding - so models supporting up to 512 tokens are usually more than enough.\n",
    "* Model size: the size of a model indicates how easy it will be to run. All models near the top of MTEB are reasonably sized. One of the largest is instructor-xl (requiring 4.96GB of memory), which we can easily run on consumer hardware.\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface/\n",
    "\n",
    "Youtube Video - https://www.youtube.com/watch?v=TcRRfcbsApw\n",
    "\n",
    "Uses [Semantic-router](https://pypi.org/project/semantic-router/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Library\n",
    "\n",
    "Semantic chunkers allow us to build more context aware chunks of information. We can use this for RAG, splitting video, audio, and much more.\n",
    "\n",
    "In this example, we will stick with a simple RAG-focused example. We will learn about three different types of chunkers available to us; StatisticalChunker, ConsecutiveChunker, CumulativeChunker, and RegexChunker.\n",
    "\n",
    "\n",
    "* https://github.com/aurelio-labs/semantic-chunkers\n",
    "* https://github.com/aurelio-labs/semantic-chunkers/blob/main/docs/00-chunkers-intro.ipynb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"Alibaba-NLP/gte-base-en-v1.5\", trust_remote_code=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[8].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Tell me 10 different new and unique findings that will help to start a new business in combating social isolation using AI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What are the findings of study 3\")\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
