{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8160381,"sourceType":"datasetVersion","datasetId":4827851}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install -U -qq sentence-transformers\n!pip install -U -qq transformers accelerate bitsandbytes keybert summa multi_rake","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:31:55.529900Z","iopub.execute_input":"2024-04-23T22:31:55.530737Z","iopub.status.idle":"2024-04-23T22:32:48.771665Z","shell.execute_reply.started":"2024-04-23T22:31:55.530702Z","shell.execute_reply":"2024-04-23T22:32:48.770455Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -qq git+https://github.com/LIAAD/yake","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:32:48.773430Z","iopub.execute_input":"2024-04-23T22:32:48.773744Z","iopub.status.idle":"2024-04-23T22:33:06.683328Z","shell.execute_reply.started":"2024-04-23T22:32:48.773716Z","shell.execute_reply":"2024-04-23T22:33:06.682235Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:33:57.959754Z","iopub.execute_input":"2024-04-23T22:33:57.960121Z","iopub.status.idle":"2024-04-23T22:34:00.643410Z","shell.execute_reply.started":"2024-04-23T22:33:57.960089Z","shell.execute_reply":"2024-04-23T22:34:00.642286Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Context**\n\nYou spend hours perfecting your resume, making sure it outlines your skills and experience in the best possible light. After all, when it comes to job hunting, your resume is your most important tool.\n\nBut after all that work, you’re still not getting enough interviews, even for jobs you know you’re qualified for. Why not?\n\nWhat you might not realize is that your resume usually doesn’t go to a human being after you submit it – it goes to a computer. In fact, there’s a good chance a real person will never see your resume!\n\nThat’s because more and more employers are using applicant tracking systems (ATS) to screen resumes. \n\nWhat is an ATS? It’s computer software designed to scan resumes for certain keywords and weed out the ones that don’t match the job description.\n\nSo if you want your resume to actually make it into the hands of a human being, you need to make sure it’s optimized for the ATS.\n\nIn this notebook, we’re going to use NLP and language models to create a resume that can \"pass\" ATS!\n\n**How applicant tracking systems work?**\nThere are 4 basic steps to how an applicant tracking system works:\n\n1. A job requisition enters into the ATS. This requisition includes information about the position, such as the job title, desired skills, and required experience.\n2. The ATS then uses this information to create a profile for the ideal candidate.\n3. As applicants submit their resumes, the ATS parses, sorts, and ranks them based on how well they match the profile.\n4. Hiring managers then quickly identify the most qualified candidates and move them forward in the hiring process.\n\nWhat’s especially important to understand is that recruiters often filter resumes by searching for key skills and job titles.\n\nThis means that if you can predict the resume keywords that recruiters will use in their search, you’ll greatly increase your chances of moving on in the hiring process. But you don’t have to guess which keywords to use. All you have to do is analyze the job description to find them.\n\nThis notebook automates this process by using AI technology to analyze resume against the job description. It then provides a score that shows how well your resume matches the job description.","metadata":{}},{"cell_type":"markdown","source":"**Who uses ATS?**\nOver 97% of Fortune 500 companies use ATS while a Kelly OCG survey estimated that 66% of large companies and 35% of small organizations rely on recruitment software. And these numbers continue to grow.\n\nIf you’re applying to a large organization, you’ll most likely face an ATS. \n\nIf you’re applying through any online form, you’re applying through an ATS. \n\nEven job sites like Indeed and LinkedIn have their own built-in ATS.\n\nIt’s clear that ATS is here to stay. That’s why it’s so important to use the right keywords and format your resume in a way that makes it easy for ATS software to read.\n\n**How to optimize your resume for an ATS?**\n1. Carefully tailor your resume to the job description every single time you apply.\n2. Optimize for ATS search and ranking algorithms by matching your resume keywords to the job description.\n3. Use both the long-form and acronym version of keywords (e.g. “Master of Business Administration (MBA)” or “Search Engine Optimization (SEO)”) for maximum searchability.\n4. Use a chronological or hybrid resume format (avoid the functional resume format).\n5. Use a traditional resume font like Helvetica, Garamond, or Georgia.\n6. Don’t use headers or footers as the information might get lost or cause a parsing error.\n7. Use standard resume section headings like “Work Experience” rather than being cute or clever (“Where I’ve Been”).\n8. Use an ATS-friendly resume builder to create your resume.","metadata":{}},{"cell_type":"markdown","source":"### Keyword Extraction\n","metadata":{}},{"cell_type":"markdown","source":"**TF-IDF**\n\nterm frequency–inverse document frequency, often abbreviated tf-idf, is a method that tries to identify the most distinctively frequent or significant words in a document. \n\nYou take the number of times a term occurs in a document (term frequency). Then you take the number of documents in which the same term occurs at least once divided by the total number of documents (document frequency), and you flip that fraction on its head (inverse document frequency). Then you multiply the two numbers together (term_frequency * inverse_document_frequency).\n\nThe reason we take the inverse, or flipped fraction, of document frequency is to boost the rarer words that occur in relatively few documents. Think about the inverse document frequency for the word “said” vs the word “pigeon.” The term “said” appears in 13 (document frequency) of 14 (total documents) Lost in the City stories (14 / 13 –> a smaller inverse document frequency) while the term “pigeons” only occurs in 2 (document frequency) of the 14 stories (total documents) (14 / 2 –> a bigger inverse document frequency, a bigger tf-idf boost).\n","metadata":{}},{"cell_type":"code","source":"linkedin_data = pd.read_excel('/kaggle/input/linkedin-sr-data-scientist-desc/linkedin_sr_data_scientist_desc.xlsx', sheet_name='Sheet1')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:03.712968Z","iopub.execute_input":"2024-04-23T22:34:03.714000Z","iopub.status.idle":"2024-04-23T22:34:04.500846Z","shell.execute_reply.started":"2024-04-23T22:34:03.713965Z","shell.execute_reply":"2024-04-23T22:34:04.499930Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Calculate tf–idf**\nTo calculate tf–idf scores for every word, we’re going to use scikit-learn’s TfidfVectorizer.\n\nWhen you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n\nThe recommended way to run TfidfVectorizer is with smoothing (smooth_idf = True) and normalization (norm='l2') turned on. These parameters will better account for differences in text length, and overall produce more meaningful tf–idf scores. Smoothing and L2 normalization are actually the default settings for TfidfVectorizer, so to turn them on, you don’t need to include any extra code at all.\n\nInitialize TfidfVectorizer with desired parameters (default smoothing and normalization)","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(stop_words='english')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:06.011099Z","iopub.execute_input":"2024-04-23T22:34:06.012145Z","iopub.status.idle":"2024-04-23T22:34:06.016354Z","shell.execute_reply.started":"2024-04-23T22:34:06.012108Z","shell.execute_reply":"2024-04-23T22:34:06.015354Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tfidf_vector = tfidf_vectorizer.fit_transform(linkedin_data['skills'])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:06.556024Z","iopub.execute_input":"2024-04-23T22:34:06.556499Z","iopub.status.idle":"2024-04-23T22:34:06.580541Z","shell.execute_reply.started":"2024-04-23T22:34:06.556470Z","shell.execute_reply":"2024-04-23T22:34:06.579811Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Make a DataFrame out of the resulting tf–idf vector, setting the “feature names” or words as columns and the titles as rows","metadata":{}},{"cell_type":"code","source":"tfidf_df = pd.DataFrame(tfidf_vector.toarray(), columns=tfidf_vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:07.287217Z","iopub.execute_input":"2024-04-23T22:34:07.288187Z","iopub.status.idle":"2024-04-23T22:34:07.294290Z","shell.execute_reply.started":"2024-04-23T22:34:07.288155Z","shell.execute_reply":"2024-04-23T22:34:07.293004Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tfidf_df = tfidf_df.stack().reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:07.750077Z","iopub.execute_input":"2024-04-23T22:34:07.750784Z","iopub.status.idle":"2024-04-23T22:34:07.762558Z","shell.execute_reply.started":"2024-04-23T22:34:07.750751Z","shell.execute_reply":"2024-04-23T22:34:07.761675Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tfidf_df = tfidf_df.drop('level_0', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:08.029057Z","iopub.execute_input":"2024-04-23T22:34:08.029477Z","iopub.status.idle":"2024-04-23T22:34:08.036081Z","shell.execute_reply.started":"2024-04-23T22:34:08.029449Z","shell.execute_reply":"2024-04-23T22:34:08.035122Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tfidf_df = tfidf_df.rename(columns={0:'tfidf','level_1': 'term'})","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:08.560299Z","iopub.execute_input":"2024-04-23T22:34:08.561125Z","iopub.status.idle":"2024-04-23T22:34:08.566094Z","shell.execute_reply.started":"2024-04-23T22:34:08.561093Z","shell.execute_reply":"2024-04-23T22:34:08.565124Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"To find out the top 10 words with the highest tf–idf","metadata":{}},{"cell_type":"code","source":"tfidf_df.sort_values(by=['tfidf'], ascending=[False]).head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:09.538615Z","iopub.execute_input":"2024-04-23T22:34:09.539348Z","iopub.status.idle":"2024-04-23T22:34:09.556956Z","shell.execute_reply.started":"2024-04-23T22:34:09.539317Z","shell.execute_reply":"2024-04-23T22:34:09.556109Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"               term     tfidf\n3470     statistics  0.422105\n3419        related  0.386528\n5590     principles  0.378246\n741         privacy  0.337159\n4431        applied  0.331959\n5274          using  0.317366\n2458          model  0.310401\n5955      languages  0.308765\n5710  understanding  0.305988\n3057       training  0.301546","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>term</th>\n      <th>tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3470</th>\n      <td>statistics</td>\n      <td>0.422105</td>\n    </tr>\n    <tr>\n      <th>3419</th>\n      <td>related</td>\n      <td>0.386528</td>\n    </tr>\n    <tr>\n      <th>5590</th>\n      <td>principles</td>\n      <td>0.378246</td>\n    </tr>\n    <tr>\n      <th>741</th>\n      <td>privacy</td>\n      <td>0.337159</td>\n    </tr>\n    <tr>\n      <th>4431</th>\n      <td>applied</td>\n      <td>0.331959</td>\n    </tr>\n    <tr>\n      <th>5274</th>\n      <td>using</td>\n      <td>0.317366</td>\n    </tr>\n    <tr>\n      <th>2458</th>\n      <td>model</td>\n      <td>0.310401</td>\n    </tr>\n    <tr>\n      <th>5955</th>\n      <td>languages</td>\n      <td>0.308765</td>\n    </tr>\n    <tr>\n      <th>5710</th>\n      <td>understanding</td>\n      <td>0.305988</td>\n    </tr>\n    <tr>\n      <th>3057</th>\n      <td>training</td>\n      <td>0.301546</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Extract Keywords using Python\n\n**YAKE!**\n\nIt is a lightweight, unsupervised automatic keyword extraction method that relies on statistical text features extracted from individual documents to identify the most relevant keywords in the text. \n\nThis system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, text size, domain, or language. Yake defines a set of five features capturing keyword characteristics which are heuristically combined to assign a single score to every keyword. The lower the score, the more significant the keyword will be.\n\nYAKE! pays attention to capital letters and gives more importance to words that start with a capital letter.","metadata":{}},{"cell_type":"code","source":"job = linkedin_data['skills'].iloc[14]","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:11.542118Z","iopub.execute_input":"2024-04-23T22:34:11.542836Z","iopub.status.idle":"2024-04-23T22:34:11.546885Z","shell.execute_reply.started":"2024-04-23T22:34:11.542803Z","shell.execute_reply":"2024-04-23T22:34:11.545981Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import yake\nkw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\nkeywords = kw_extractor.extract_keywords(job)\nfor kw, v in keywords:\n  print(\"Keyphrase: \",kw, \": score\", v)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:14.719638Z","iopub.execute_input":"2024-04-23T22:34:14.720292Z","iopub.status.idle":"2024-04-23T22:34:15.663500Z","shell.execute_reply.started":"2024-04-23T22:34:14.720249Z","shell.execute_reply":"2024-04-23T22:34:15.662446Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Keyphrase:  analyzing data : score 0.021871602942235096\nKeyphrase:  experience working : score 0.09598042658969005\nKeyphrase:  years : score 0.14629926398739557\nKeyphrase:  analyzing : score 0.14629926398739557\nKeyphrase:  data : score 0.14629926398739557\nKeyphrase:  experience : score 0.14748481196459576\nKeyphrase:  Safety : score 0.14954886395715059\nKeyphrase:  Fraud : score 0.14954886395715059\nKeyphrase:  Spam : score 0.14954886395715059\nKeyphrase:  Investigations : score 0.14954886395715059\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Rake**\n\nRake is short for Rapid Automatic Keyword Extraction and it is a method of extracting keywords from individual documents. It can also be applied to new fields very easily and is very effective in dealing with multiple types of documents, especially text that requires specific grammatical conventions. Rake identifies key phrases in a text by analyzing the occurrence of a word and its compatibility with other words in the text (co-occurrence).","metadata":{}},{"cell_type":"code","source":"from multi_rake import Rake\nrake = Rake()\nkeywords = rake.apply(job)\nprint(keywords[:10])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:20.912461Z","iopub.execute_input":"2024-04-23T22:34:20.913246Z","iopub.status.idle":"2024-04-23T22:34:20.944093Z","shell.execute_reply.started":"2024-04-23T22:34:20.913212Z","shell.execute_reply":"2024-04-23T22:34:20.943294Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[('machine learning lifecycle', 9.0), ('communicate complex concepts', 9.0), ('relevant work experience', 8.0), ('2+ years', 4.0), ('experience working', 4.0), ('cyber proficiency', 4.0), ('programming language', 4.0), ('basic knowledge', 4.0), ('cross-functional stakeholders', 4.0), ('experience', 2.0)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**TextRank**\n\nTextRank is an unsupervised method for extracting keywords and sentences. It is based on a graph where each node is a word, and edges represent relationships between words which are formed by defining the co-occurrence of words within a moving window of a predetermined size. The algorithm is inspired by PageRank which was used by Google to rank websites. \n\nIt first Tokenizes and annotates text with Part of Speech (PoS). It only considers single words. However, no n-grams are used, multi-words are reconstructed later. An edge is created if lexical units co-occur within a window of N-words to obtain an unweighted undirected graph. \n\nThen it runs the text rank algorithm to rank the words. The most important lexical words are selected and then adjacent keywords are folded into a multi-word keyword.","metadata":{}},{"cell_type":"code","source":"from summa import keywords\nTR_keywords = keywords.keywords(job, scores=True)\nprint(TR_keywords[0:10])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:25.945004Z","iopub.execute_input":"2024-04-23T22:34:25.945957Z","iopub.status.idle":"2024-04-23T22:34:25.977828Z","shell.execute_reply.started":"2024-04-23T22:34:25.945921Z","shell.execute_reply":"2024-04-23T22:34:25.976936Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[('experience working', 0.6087697185571863), ('relevant work', 0.6087697185571861), ('basic', 0.05370732274465505), ('language', 0.049357038879681674), ('r', 0.026853661372327527)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**KeyBert**\n\nKeyBERT is a simple, easy-to-use keyword extraction algorithm that takes advantage of SBERT embeddings to generate keywords and key phrases from a document that are more similar to the document. First, document embedding (a representation) is generated using the sentences-BERT model. \n\nNext, the embeddings of words are extracted for N-gram phrases. The similarity of each keyphrase to the document is then measured using cosine similarity. The most similar words can then be identified as the words that best describe the entire document and are considered as keywords.","metadata":{}},{"cell_type":"code","source":"from keybert import KeyBERT\nkw_model = KeyBERT(model='all-mpnet-base-v2')\nkeywords = kw_model.extract_keywords(job, \n\n                                     keyphrase_ngram_range=(1, 3), \n\n                                     stop_words='english', \n\n                                     highlight=False,\n\n                                     top_n=10)\n\nkeywords_list= list(dict(keywords).keys())\n\nprint(keywords_list)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:28.070059Z","iopub.execute_input":"2024-04-23T22:34:28.070427Z","iopub.status.idle":"2024-04-23T22:34:50.561344Z","shell.execute_reply.started":"2024-04-23T22:34:28.070396Z","shell.execute_reply":"2024-04-23T22:34:50.560161Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"2024-04-23 22:34:35.113868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-23 22:34:35.114011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-23 22:34:35.236419: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9414b7b27448a996e506da5610422e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ef8e2521b74c4aa9158ef411bf2198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39833b87c3804bc48d3f911f4db0da1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a887ef430c4b45b9e70ccfc08ac6de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6504e274462e44c2b21d42773428cd0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b659c84000c044bfb1ce2902625d16c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6550660eb1b44d8fb293fe8108a9be56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1947180723084c58a5477fbf6e658e1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2df3b4e3ac3c4c8bb63c3bf03c0c10f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a7707870ea4817ab8804971e1d83f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae16c8c2bbbe4982be8134b9430fafd2"}},"metadata":{}},{"name":"stdout","text":"['experience working analyzing', 'years experience working', 'data technical background', 'technical background', 'machine learning lifecycle', 'investigations cyber proficiency', 'relevant work experience', 'cyber proficiency sql', 'experience working', 'machine learning models']\n","output_type":"stream"}]},{"cell_type":"code","source":"keywords = kw_model.extract_keywords(job, \n\n                                     keyphrase_ngram_range=(1, 3), \n\n                                     stop_words='english', \n\n                                     highlight=True,\n\n                                     top_n=10)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:34:50.563738Z","iopub.execute_input":"2024-04-23T22:34:50.564536Z","iopub.status.idle":"2024-04-23T22:34:50.724749Z","shell.execute_reply.started":"2024-04-23T22:34:50.564500Z","shell.execute_reply":"2024-04-23T22:34:50.723574Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"years of \u001b[30;48;2;255;255;0mexperience working\u001b[0m with analyzing \u001b[30;48;2;255;255;0mdata Technical background\u001b[0m \u001b[30;48;2;255;255;0mrelevant work experience\u001b[0m Trust Safety Fraud \nSpam Investigations or Cyber Proficiency in SQL and at least one programming language Python or Basic knowledge of \n\u001b[30;48;2;255;255;0mmachine learning lifecycle\u001b[0m experience with implementing or improving \u001b[30;48;2;255;255;0mmachine learning models\u001b[0m to communicate complex\nconcepts clearly to cross functional stakeholders\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">years of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">experience working</span> with analyzing <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">data Technical background</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">relevant work experience</span> Trust Safety Fraud \nSpam Investigations or Cyber Proficiency in SQL and at least one programming language Python or Basic knowledge of \n<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">machine learning lifecycle</span> experience with implementing or improving <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">machine learning models</span> to communicate complex\nconcepts clearly to cross functional stakeholders\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"keywords = kw_model.extract_keywords(job, \n\n                                     keyphrase_ngram_range=(1, 2), \n\n                                     stop_words='english', \n\n                                     highlight=True,\n\n                                     top_n=10)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:35:24.814494Z","iopub.execute_input":"2024-04-23T22:35:24.815253Z","iopub.status.idle":"2024-04-23T22:35:24.886474Z","shell.execute_reply.started":"2024-04-23T22:35:24.815222Z","shell.execute_reply":"2024-04-23T22:35:24.885320Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"years of \u001b[30;48;2;255;255;0mexperience working\u001b[0m analyzing \u001b[30;48;2;255;255;0mdata Technical\u001b[0m or relevant \u001b[30;48;2;255;255;0mwork experience\u001b[0m Trust Safety Fraud Spam \nInvestigations or \u001b[30;48;2;255;255;0mCyber Proficiency\u001b[0m SQL and at least one programming language Python or Basic knowledge of \u001b[30;48;2;255;255;0mmachine \u001b[0m\n\u001b[30;48;2;255;255;0mlearning\u001b[0m and experience with implementing or improving \u001b[30;48;2;255;255;0mmachine learning\u001b[0m Ability to communicate complex concepts \nclearly to cross functional stakeholders\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">years of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">experience working</span> analyzing <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">data Technical</span> or relevant <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">work experience</span> Trust Safety Fraud Spam \nInvestigations or <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Cyber Proficiency</span> SQL and at least one programming language Python or Basic knowledge of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">machine </span>\n<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">learning</span> and experience with implementing or improving <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">machine learning</span> Ability to communicate complex concepts \nclearly to cross functional stakeholders\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"### Extracting Job Skills using LLMs\n\nTF-IDF needs a lot of processing and does not work as efficently. Classical NLP tools do not extract all the key words.\n\nThe idea of extracting keywords from documents through an LLM is straightforward and allows for easily testing your LLM and its capabilities.\n\n**Why Mistral 7B model?**\nMistral 7B is a 7-billion-parameter language model released by Mistral AI. Mistral 7B is a carefully designed language model that provides both efficiency and high performance to enable real-world applications. Due to its efficiency improvements, the model is suitable for real-time applications where quick responses are essential.\n\nMistral 7B has demonstrated superior performance across various benchmarks, outperforming even models with larger parameter counts. It excels in areas like mathematics, code generation, and reasoning.\n\n* https://www.linkedin.com/pulse/proof-concept-using-large-language-models-llms-extract-truc-phan-w5vde/\n* https://huggingface.co/docs/transformers/main/en/model_doc/llama2\n* https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n* https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing\n* https://towardsdatascience.com/meta-llama-3-optimized-cpu-inference-with-hugging-face-and-pytorch-9dde2926be5c\n* https://www.promptingguide.ai/models/mistral-7b","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login, Repository\n\n# Login to Hugging Face\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:35:31.930830Z","iopub.execute_input":"2024-04-23T22:35:31.931203Z","iopub.status.idle":"2024-04-23T22:35:31.958975Z","shell.execute_reply.started":"2024-04-23T22:35:31.931172Z","shell.execute_reply":"2024-04-23T22:35:31.958180Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bdbe28672fd4b15a8b3e81179fee0c0"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import BitsAndBytesConfig\nimport torch\nimport time\nimport regex\nimport json\n\nstart = time.time()\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n#token = TOKEN\n\nprompt = f\"\"\"\n\nAlways assist with care, respect, and truth. \nRespond with utmost utility yet securely. \nAvoid harmful, unethical, prejudiced, or negative content. \nEnsure replies promote fairness and positivity.\n\nRead the JOB DESCRIPTION \nand extract keywords for \ndata related skills such as Generative AI, Statistics,\nmachine learning tasks such as feature enginerring, classification,\ndata analysis tools such as Tableau, Streamlit, \nprogramming languages such as Python, SQL,\neducation such as B.Sc.,\nand soft skills e.g., communication required.\n\nGenerate a valid JSON object with following key artifacts\nskills: \"\",\nmachine learning: \"\"\ntools: \"\",\nlanguages: \"\",\neducation: \"\",\nsoft skills : \"\".\n\nJust generate the JSON object without explanation, unique words or duplicates. Be brief.\n\n\nJOB DESCRIPTION\n\"\n{job}\n\"\n\n\"\"\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    llm_int8_enable_fp32_cpu_offload=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n                                             device_map=\"auto\", \n                                             quantization_config=bnb_config)\n                                             #token=TOKEN\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          use_fast=True, \n                                          quantization_config=bnb_config)\n                                          #token=TOKEN\n    \nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n\noutput = model.generate(**model_inputs,\n                          max_new_tokens=1024,\n                          repetition_penalty=1.5)\n\ntext_string = tokenizer.decode(output[0], \n                       skip_special_tokens=True)\n\n# Define a pattern to match JSON object\npattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')\n\n# Find JSON object using regular expression\njson_match = pattern.findall(text_string)\n\nif json_match:\n    # Clean the text\n    cleaned_text = str(json_match).strip('[]')  # remove square brackets\n    cleaned_text = cleaned_text.strip()  # remove leading/trailing spaces\n    cleaned_text = cleaned_text.replace('\\n', '')  # remove newlines\n    cleaned_text = cleaned_text.replace('\\t', '')  # remove tabs\n    cleaned_text = cleaned_text.replace('\\\\', '')  # remove escape characters\n\n    # Print the dictionary\n    print(cleaned_text)\nelse:\n    print(\"No JSON object found in the text.\")\n    \nend = time.time()\nprint(f\"Time (minutes): {(end - start)}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:35:57.268595Z","iopub.execute_input":"2024-04-23T22:35:57.269311Z","iopub.status.idle":"2024-04-23T22:37:29.601787Z","shell.execute_reply.started":"2024-04-23T22:35:57.269271Z","shell.execute_reply":"2024-04-23T22:37:29.600750Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"973a6975d95d4e6d89883c0485b66de2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2049a2a1678447bcb2dc4aaab172f4d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dd4b782d02b4f85917722775be15757"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b79ef149cf36482c8c85d26ca98b7f6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e77901d72b460a8333f178065431e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f59253627a3a43a48096eeb86e8dcb4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ee7dfee9654d35a356ccbc0ed535ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1b36b6578cd4dc2b070b2a3441ce91d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cfb71108a014825bc96aac068673f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e46805b9845445ddbcbf5710d3a52dc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e01de9ca214731934282a39c7dbbc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673f988091f742579d86a9afd18f8816"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"'{nt\"skills\": [\"Data Analysis\", \"SQL\",\"Programming\"],n    \"machine_learning\":[\"Machine Learning Model Implementation/Improvement\"],n     \"tools\":\"N/A\",n      \"languages\":{\"python\":\"1\"},n       \"education\": {\"Bsc Degree\":\"0\"},\"n        software Skills \": {   \"communicationSkillLevel\": \"Advanced\" }n}'\nTime (minutes): 92.3208920955658\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Semantic Similarity \nIt is the similarity between two words or two sentences/phrase/text. It measures how close or how different the two pieces of word or text are in terms of their meaning and context.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n\ndef semantic_similarity_sbert_base_v2(job,resume, model):\n    \"\"\"calculate similarity with SBERT all-mpnet-base-v2\"\"\"\n    model = SentenceTransformer(model)\n    # Compute embedding for both lists\n    embeddings1 = model.encode(job, convert_to_tensor=True)\n    embeddings2 = model.encode(resume, convert_to_tensor=True)\n\n    # Compute cosine-similarities\n    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n     \n    return cosine_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resume = \"\"\"\n\n\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"semantic_similarity_sbert_base_v2(job,resume, 'all-MiniLM-L12-v1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### How to Conduct Candidate Analysis\n\nCrafting a comprehensive candidate analysis involves multiple dimensions. Here are the key steps:\n\n1. Resume Keyword Matching: Verify alignment between the resume and job description by looking for overlaps in skills and experience.\n2. Competency-Based Evaluation: Scrutinize past achievements using the STAR technique to ensure competencies match those necessary for the role.\n3. AIDA Cover Letter Review: Evaluate the cover letter to see if it effectively grabs Attention, maintains Interest, builds Desire, and prompts Action.\n4. Fit/Gap Analysis: Determine where a candidate’s skills meet the job prerequisites and where they don't to assess overall compatibility.\n5. Growth Potential Assessment: Consider the candidate's past trajectories to estimate their potential for future growth within your startup.","metadata":{}},{"cell_type":"markdown","source":"**ChatGPT Prompt for Founders to Create Candidate Analysis**\n\n\nUsing the job description provided, I need a detailed **candidate analysis report** for a job application. This report will assist me in making an informed decision about whether to proceed with the interview process for this candidate.\n\nHere is the job description to use as a benchmark:\n\n[Job Description]\n\n#### Candidate’s Application Analysis:\n\n1. **Resume Keyword Match**: Examine the applicant's resume and extract key skills, experiences, and qualifications. Present these in a bullet-pointed list and note which directly match the job description criteria.\n\n2. **Competency-Based Evaluation**: Analyze the candidate's strongest work achievements. Use the STAR technique to break these down and comment on how these achievements demonstrate competencies required for the job.\n\n3. **Cover Letter AIDA Assessment**: Critique the cover letter using the AIDA model, focusing on how the candidate uses it to illustrate suitability for the role.\n\n4. **Fit-Gap Analysis**: Conduct a fit-gap analysis by creating two lists: one showing where the candidate's skills and experiences match the job requirements ('Fit') and another where they do not align ('Gap').\n\n5. **Growth Potential**: Comment briefly on the candidate's potential for growth and learning within the company based on their career trajectory and achievements presented.\n\n6. **Final Suitability Statement**: Conclude with a suitability statement summarizing whether the candidate should be considered for the role  based on criteria matches, potential growth, and overall fit for the company culture.\n\nPlease present your findings in a cohesive markdown format, ensuring each section is clear and well-structured for ease of review.\n\nCandidate's Application:\n\n[Candidate Application]\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}