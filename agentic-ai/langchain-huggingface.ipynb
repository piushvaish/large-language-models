{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installation\nWe need to install the required libraries to get started with different ways to use HuggingFace on Langchain.","metadata":{}},{"cell_type":"markdown","source":"Definitions:\nHugging Face: It is a leading platform providing pre-trained models and libraries for natural language understanding. Renowned for its Transformers library, Hugging Face offers an extensive collection of pre-trained models that can be fine-tuned for specific NLP tasks.\n\nLangchain: A powerful linguistic toolkit designed to facilitate various NLP tasks. Langchain encompasses functionalities for tokenization, lemmatization, part-of-speech tagging, and syntactic analysis, providing a comprehensive suite for linguistic analysis.\n\nAdvantages of Integration:\n\n1. Enhanced Linguistic Analysis: The amalgamation of Langchain's linguistic toolkit with Hugging Face's transformer models allows for a deeper analysis of text, leveraging both syntactic and semantic understanding.\n\n2. Extended Functionalities: Integrating Langchain with Hugging Face provides access to advanced tokenization, lemmatization, and other linguistic processing methods, enabling a more nuanced understanding of language structures.\n\n3. Optimized NLP Pipelines: By leveraging the strengths of both platforms, users can construct optimized NLP pipelines that efficiently handle a wide array of tasks, from text classification to machine translation.\n\n4. Flexibility in Model Deployment: The integration enables seamless deployment of combined models, allowing for more flexibility in handling diverse NLP tasks within a unified framework.","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install sentence-transformers\n!pip install bitsandbytes accelerate\n!pip install langchain\n!pip install llama-cpp-python","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:38:04.177148Z","iopub.execute_input":"2024-04-06T21:38:04.177534Z","iopub.status.idle":"2024-04-06T21:40:18.916274Z","shell.execute_reply.started":"2024-04-06T21:38:04.177498Z","shell.execute_reply":"2024-04-06T21:40:18.915325Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (2.6.1)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.21.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.21.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nRequirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.1.14)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.30 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.31)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.37 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.40)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.1)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.40)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.37->langchain) (23.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.2.60.tar.gz (37.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.9.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.60-cp310-cp310-manylinux_2_31_x86_64.whl size=2948750 sha256=76fe8c8c1ecdd991e0b8243cdf6f21f1b36fe5a1c9d6f9fa48fd63df94980648\n  Stored in directory: /root/.cache/pip/wheels/ad/7b/02/5200ea3612eca540182654a0b72f0b2a90c0f32d1633c931e4\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.60\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Approach 1: HuggingFace Pipeline\nThe pipelines are a great and easy way to use models for inference. HuggingFace provides a pipeline wrapper class that can easily integrate tasks like text generation and summarization in just one line of code. This code line contains the calling pipeline attribute by instantiating the model, tokenizer, and task name.\n\nWe must load the Large Langauge model and relevant tokenizer to implement this. Since not everyone can access A100 or V100 GPUs, we must proceed with the Free T4 GPU. To run the large language model for inference using pipeline, we will use orca-mini 3 billion parameter LLM with quantization configuration to reduce the model size.","metadata":{}},{"cell_type":"code","source":"from langchain.llms.huggingface_pipeline import HuggingFacePipeline\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \nfrom transformers import BitsAndBytesConfig\n\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:40:43.381257Z","iopub.execute_input":"2024-04-06T21:40:43.381994Z","iopub.status.idle":"2024-04-06T21:41:03.188804Z","shell.execute_reply.started":"2024-04-06T21:40:43.381961Z","shell.execute_reply":"2024-04-06T21:41:03.188018Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-04-06 21:40:50.962048: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-06 21:40:50.962154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-06 21:40:51.081372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the provided code snippet, we utilize AutoModelForCausalLM to load the model and AutoTokenizer to load the tokenizer. Once the model and tokenizer are loaded, assign the model and tokenizer to the pipeline and mention the task to be text generation. The pipeline also allows adjustment of the output sequence length by modifying max_new_tokens.","metadata":{}},{"cell_type":"code","source":"model_id = \"pankajmathur/orca_mini_3b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n                 model_id,\n                 quantization_config=nf4_config\n                 )\npipe = pipeline(\"text-generation\", \n               model=model, \n               tokenizer=tokenizer, \n               max_new_tokens=512\n               )","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:48:33.755365Z","iopub.execute_input":"2024-04-06T21:48:33.756361Z","iopub.status.idle":"2024-04-06T21:52:20.712576Z","shell.execute_reply.started":"2024-04-06T21:48:33.756318Z","shell.execute_reply":"2024-04-06T21:52:20.711719Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7899db7fc67421a971b99d3435afd3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/534k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf87124cfd004407af729190708d5f3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.98M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef9bf776b894631b6bc2bb9b87ada8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/208 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408f340df4ad46b4945399948bec2b31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2d766853b74127919575db936a26df"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/22.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fd1771e092a4c2a93b89b9dbffa26ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84926520328c467ab243d8aee0ff31f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9c686e38f124b19b776cf417d811553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"217bf63f5dfd41c9927c262caee24b40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6aba3431e9647db80625a6a3a9059ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"091452df869f4790a6cf8ce815f82b13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"703f528f2bba45b1b098982ea4a0a14c"}},"metadata":{}}]},{"cell_type":"markdown","source":"Good job on running the pipeline successfully. HuggingFacePipeline wrapper class helps to integrate the Transformers model and Langchain. The code snippet below defines the prompt template for the orca model.","metadata":{}},{"cell_type":"code","source":"hf = HuggingFacePipeline(pipeline=pipe)\n\nquery = \"Where is Atrani?\"\n\nprompt = f\"\"\"\n### System:\nYou are an AI assistant that follows instruction extremely well. \nHelp as much as you can. Please be truthful and give direct answers\n\n### User:\n{query}\n\n### Response:\n\"\"\"\n\nresponse = hf.predict(prompt)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:52:20.714173Z","iopub.execute_input":"2024-04-06T21:52:20.714500Z","iopub.status.idle":"2024-04-06T21:52:25.282466Z","shell.execute_reply.started":"2024-04-06T21:52:20.714472Z","shell.execute_reply":"2024-04-06T21:52:25.281456Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"},{"name":"stdout","text":"\n### System:\nYou are an AI assistant that follows instruction extremely well. \nHelp as much as you can. Please be truthful and give direct answers\n\n### User:\nWhere is Atrani?\n\n### Response:\n I'm sorry, but I couldn't find any location for Atrani. Could you please provide more information or clarify your question?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Gated Models","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:00:45.611345Z","iopub.execute_input":"2024-04-06T22:00:45.611746Z","iopub.status.idle":"2024-04-06T22:00:53.968321Z","shell.execute_reply.started":"2024-04-06T22:00:45.611716Z","shell.execute_reply":"2024-04-06T22:00:53.967349Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8dae880e7464f3d986ded9c1ff5d0b4"}},"metadata":{}},{"name":"stdout","text":"<bos>Write me a poem about Machine Learning.\n\nMachines, they weave and they learn,\nFrom\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Approach 2: HuggingFace Hub using Inference API\nIn approach one, you might have noticed that while using the pipeline, the model and tokenization download and load the weights. This approach might be time-consuming if the length of the model is enormous. Thus, the HuggingFace Hub Inference API comes in handy. To integrate HuggingFace Hub with Langchain, one requires a HuggingFace Access Token.\n\nSteps to get HuggingFace Access Token\n1. Log in to HuggingFace.co.\n2. Click on your profile icon at the top-right corner, then choose “Settings.”\n3. In the left sidebar, navigate to “Access Token.”\n4. Generate a new access token, assigning it the “write” role.","metadata":{}},{"cell_type":"code","source":"from langchain.llms import HuggingFaceHub\nimport os\nfrom getpass import getpass\n\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"HF Token:\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:53:53.408841Z","iopub.execute_input":"2024-04-06T21:53:53.409235Z","iopub.status.idle":"2024-04-06T21:54:03.607988Z","shell.execute_reply.started":"2024-04-06T21:53:53.409204Z","shell.execute_reply":"2024-04-06T21:54:03.607130Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdin","text":"HF Token: ·····································\n"}]},{"cell_type":"code","source":"llm = HuggingFaceHub(\n    repo_id=\"google/gemma-2b-it\", \n    model_kwargs={\"temperature\": 0.5, \"max_length\": 64,\"max_new_tokens\":512}\n)\n\nquery = \"Where is Atrani?\"\n\nprompt = f\"\"\"\n <|system|>\nYou are an AI assistant that follows instruction extremely well.\nPlease be truthful and give direct answers\n</s>\n <|user|>\n {query}\n </s>\n <|assistant|>\n\"\"\"\n\nresponse = llm.predict(prompt)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:01:56.743810Z","iopub.execute_input":"2024-04-06T22:01:56.744164Z","iopub.status.idle":"2024-04-06T22:01:57.075749Z","shell.execute_reply.started":"2024-04-06T22:01:56.744135Z","shell.execute_reply":"2024-04-06T22:01:57.074691Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\n <|system|>\nYou are an AI assistant that follows instruction extremely well.\nPlease be truthful and give direct answers\n</s>\n <|user|>\n Where is Atrani?\n </s>\n <|assistant|>\n I do not have access to real-time information and cannot provide location details.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Approach 3: LlamaCPP\nLLamaCPP allows the use of models packaged as. gguf files format that runs efficiently in CPU-only and mixed CPU/GPU environments using the llama.\n\nTo use LlamaCPP, we specifically need models whose model_path ends with gguf. You can download the model from here: zephyr-7b-beta.Q4.gguf. Once this model is downloaded, you can directly upload it to your drive or any other local storage.","metadata":{}},{"cell_type":"markdown","source":"### Models are downloaded","metadata":{}},{"cell_type":"code","source":"#!ls ~/.cache/huggingface/hub","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:07:59.268336Z","iopub.execute_input":"2024-04-06T22:07:59.268754Z","iopub.status.idle":"2024-04-06T22:08:00.291830Z","shell.execute_reply.started":"2024-04-06T22:07:59.268715Z","shell.execute_reply":"2024-04-06T22:08:00.290653Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"models--google--gemma-2b-it  models--pankajmathur--orca_mini_3b  version.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### download-huggingface-models","metadata":{}},{"cell_type":"code","source":"#!sudo apt-get install git-lfs","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:10:59.494327Z","iopub.execute_input":"2024-04-06T22:10:59.495215Z","iopub.status.idle":"2024-04-06T22:11:02.735765Z","shell.execute_reply.started":"2024-04-06T22:10:59.495164Z","shell.execute_reply":"2024-04-06T22:11:02.734593Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.9.2-1).\n0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git lfs install","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:11:37.568971Z","iopub.execute_input":"2024-04-06T22:11:37.569409Z","iopub.status.idle":"2024-04-06T22:11:38.623250Z","shell.execute_reply.started":"2024-04-06T22:11:37.569371Z","shell.execute_reply":"2024-04-06T22:11:38.622081Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Error: Failed to call git rev-parse --git-dir: exit status 128 \nGit LFS initialized.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:14:07.806213Z","iopub.execute_input":"2024-04-06T22:14:07.807196Z","iopub.status.idle":"2024-04-06T22:15:46.462756Z","shell.execute_reply.started":"2024-04-06T22:14:07.807153Z","shell.execute_reply":"2024-04-06T22:15:46.461717Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Cloning into 'zephyr-7B-beta-GGUF'...\nremote: Enumerating objects: 60, done.\u001b[K\nremote: Total 60 (delta 0), reused 0 (delta 0), pack-reused 60\u001b[K\nUnpacking objects: 100% (60/60), 19.09 KiB | 1.19 MiB/s, done.\nDownloading zephyr-7b-beta.Q2_K.gguf (3.1 GB)\nError downloading object: zephyr-7b-beta.Q2_K.gguf (2b77579): Smudge error: Error downloading zephyr-7b-beta.Q2_K.gguf (2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149): cannot write data to tempfile \"/kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/incomplete/2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149969138544\": write /kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/incomplete/2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149969138544: no space left on device\nUnable to log panic to /kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/logs: mkdir /kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/logs: no space left on device\n\ngit-lfs/2.9.2 (GitHub; linux amd64; go 1.13.5)\ngit version 2.25.1\n\n$ git-lfs filter-process\nError downloading object: zephyr-7b-beta.Q2_K.gguf (2b77579): Smudge error: Error downloading zephyr-7b-beta.Q2_K.gguf (2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149): cannot write data to tempfile \"/kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/incomplete/2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149969138544\": write /kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/incomplete/2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149969138544: no space left on device\n\nwrite /kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/incomplete/2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149969138544: no space left on device\ncannot write data to tempfile \"/kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/incomplete/2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149969138544\"\ngithub.com/git-lfs/git-lfs/errors.newWrappedError\n\tgithub.com/git-lfs/git-lfs/errors/types.go:198\ngithub.com/git-lfs/git-lfs/errors.Wrapf\n\tgithub.com/git-lfs/git-lfs/errors/errors.go:85\ngithub.com/git-lfs/git-lfs/tq.(*basicDownloadAdapter).download\n\tgithub.com/git-lfs/git-lfs/tq/basic_download.go:250\ngithub.com/git-lfs/git-lfs/tq.(*basicDownloadAdapter).DoTransfer\n\tgithub.com/git-lfs/git-lfs/tq/basic_download.go:101\ngithub.com/git-lfs/git-lfs/tq.(*adapterBase).worker\n\tgithub.com/git-lfs/git-lfs/tq/adapterbase.go:182\nruntime.goexit\n\t/usr/lib/go-1.13/src/runtime/asm_amd64.s:1357\nError downloading zephyr-7b-beta.Q2_K.gguf (2b77579c3145506bc8239390aaee138f7e2b764ab4081e6fa9bfe01a9b531149)\ngithub.com/git-lfs/git-lfs/errors.newWrappedError\n\tgithub.com/git-lfs/git-lfs/errors/types.go:198\ngithub.com/git-lfs/git-lfs/errors.Wrapf\n\tgithub.com/git-lfs/git-lfs/errors/errors.go:85\ngithub.com/git-lfs/git-lfs/lfs.(*GitFilter).downloadFile\n\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:115\ngithub.com/git-lfs/git-lfs/lfs.(*GitFilter).Smudge\n\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:76\ngithub.com/git-lfs/git-lfs/commands.smudge\n\tgithub.com/git-lfs/git-lfs/commands/command_smudge.go:127\ngithub.com/git-lfs/git-lfs/commands.filterCommand\n\tgithub.com/git-lfs/git-lfs/commands/command_filter_process.go:118\ngithub.com/spf13/cobra.(*Command).execute\n\tgithub.com/spf13/cobra/command.go:766\ngithub.com/spf13/cobra.(*Command).ExecuteC\n\tgithub.com/spf13/cobra/command.go:850\ngithub.com/spf13/cobra.(*Command).Execute\n\tgithub.com/spf13/cobra/command.go:800\ngithub.com/git-lfs/git-lfs/commands.Run\n\tgithub.com/git-lfs/git-lfs/commands/run.go:97\nmain.main\n\tgithub.com/git-lfs/git-lfs/git-lfs.go:33\nruntime.main\n\t/usr/lib/go-1.13/src/runtime/proc.go:203\nruntime.goexit\n\t/usr/lib/go-1.13/src/runtime/asm_amd64.s:1357\nSmudge error\ngithub.com/git-lfs/git-lfs/errors.newWrappedError\n\tgithub.com/git-lfs/git-lfs/errors/types.go:198\ngithub.com/git-lfs/git-lfs/errors.NewSmudgeError\n\tgithub.com/git-lfs/git-lfs/errors/types.go:284\ngithub.com/git-lfs/git-lfs/lfs.(*GitFilter).Smudge\n\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:85\ngithub.com/git-lfs/git-lfs/commands.smudge\n\tgithub.com/git-lfs/git-lfs/commands/command_smudge.go:127\ngithub.com/git-lfs/git-lfs/commands.filterCommand\n\tgithub.com/git-lfs/git-lfs/commands/command_filter_process.go:118\ngithub.com/spf13/cobra.(*Command).execute\n\tgithub.com/spf13/cobra/command.go:766\ngithub.com/spf13/cobra.(*Command).ExecuteC\n\tgithub.com/spf13/cobra/command.go:850\ngithub.com/spf13/cobra.(*Command).Execute\n\tgithub.com/spf13/cobra/command.go:800\ngithub.com/git-lfs/git-lfs/commands.Run\n\tgithub.com/git-lfs/git-lfs/commands/run.go:97\nmain.main\n\tgithub.com/git-lfs/git-lfs/git-lfs.go:33\nruntime.main\n\t/usr/lib/go-1.13/src/runtime/proc.go:203\nruntime.goexit\n\t/usr/lib/go-1.13/src/runtime/asm_amd64.s:1357\n\nCurrent time in UTC: \n2024-04-06 22:15:46\n\nENV:\nLocalWorkingDir=/kaggle/working/zephyr-7B-beta-GGUF\nLocalGitDir=/kaggle/working/zephyr-7B-beta-GGUF/.git\nLocalGitStorageDir=/kaggle/working/zephyr-7B-beta-GGUF/.git\nLocalMediaDir=/kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/objects\nLocalReferenceDirs=\nTempDir=/kaggle/working/zephyr-7B-beta-GGUF/.git/lfs/tmp\nConcurrentTransfers=3\nTusTransfers=false\nBasicTransfersOnly=false\nSkipDownloadErrors=false\nFetchRecentAlways=false\nFetchRecentRefsDays=7\nFetchRecentCommitsDays=0\nFetchRecentRefsIncludeRemotes=true\nPruneOffsetDays=3\nPruneVerifyRemoteAlways=false\nPruneRemoteName=origin\nLfsStorageDir=/kaggle/working/zephyr-7B-beta-GGUF/.git/lfs\nAccessDownload=none\nAccessUpload=none\nDownloadTransfers=basic,lfs-standalone-file\nUploadTransfers=basic,lfs-standalone-file\nGIT_EXEC_PATH=/usr/lib/git-core\nGIT_COMMIT=bacb6e67ee517a35899e366f26a789c6f3f6b389\nGIT_DIR=/kaggle/working/zephyr-7B-beta-GGUF/.git\nGIT_PYTHON_REFRESH=quiet\nGIT_PAGER=cat\n\nClient IP addresses:\n172.19.2.2\nerror: external filter 'git-lfs filter-process' failed\nfatal: zephyr-7b-beta.Q2_K.gguf: smudge filter lfs failed\nwarning: Clone succeeded, but checkout failed.\nYou can inspect what was checked out with 'git status'\nand retry with 'git restore --source=HEAD :/'\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls ~/.cache/huggingface/transformers/","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:16:21.289437Z","iopub.execute_input":"2024-04-06T22:16:21.289872Z","iopub.status.idle":"2024-04-06T22:16:22.314849Z","shell.execute_reply.started":"2024-04-06T22:16:21.289831Z","shell.execute_reply":"2024-04-06T22:16:22.313823Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"ls: cannot access '/root/.cache/huggingface/transformers/': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/lib/kaggle","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:15:47.578065Z","iopub.execute_input":"2024-04-06T22:15:47.578539Z","iopub.status.idle":"2024-04-06T22:15:48.714453Z","shell.execute_reply.started":"2024-04-06T22:15:47.578494Z","shell.execute_reply":"2024-04-06T22:15:48.713176Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"gcp.py\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.llms import LlamaCpp\n\n\nllm_cpp = LlamaCpp(\n            streaming = True,\n            model_path=\"/content/drive/MyDrive/LLM_Model/zephyr-7b-beta.Q4_K_M.gguf\",\n            n_gpu_layers=2,\n            n_batch=512,\n            temperature=0.75,\n            top_p=1,\n            verbose=True,\n            n_ctx=4096\n            )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"query = \"Who is Elon Musk?\"\n\nprompt = f\"\"\"\n <|system|>\nYou are an AI assistant that follows instruction extremely well.\nPlease be truthful and give direct answers\n</s>\n <|user|>\n {query}\n </s>\n <|assistant|>\n\"\"\"\n\nresponse = llm_cpp.predict(prompt)\nprint(response)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\nTo conclude, we successfully implemented HuggingFace and Langchain open-source models with Langchain. Using these approaches, one can easily avoid paying OpenAI API credits. This guide mainly focused on using the Open Source LLMs, one major RAG pipeline component.\n\n#### Key Takeaways\n\nUsing HuggingFace’s Transformers pipeline, one can easily pick any top-performing Large Language models, Llama2 70B, Falcon 180 B, or Mistral 7B. The inference script is less than five lines of code.\nAs not all can afford to use A100 or V100 GPUs, HuggingFace provides Free Inference API (Access Token) to implement a few models from HuggingFace Hub. The most preferred model in this case is the 7B model.\nLLamaCPP is used when you need to run Large Language models on the CPU. Currently, LlamaCPP is only supported with gguf model files.\nIt is recommended to follow the prompt template to run the predict() method on the user query.\n\n#### Reference\n* https://python.langchain.com/docs/integrations/llms/huggingface_hub\n* https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n* https://python.langchain.com/docs/integrations/llms/llamacpp\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}